\chapter{Implementation in seL4}
\label{chap:implementation}

In the previous chapter we presented our mechanisms for temporal isolation and safe resource sharing
in a high-assurance microkernel. 
Now we delve into the implementation details of scheduling contexts, scheduling context donation,
passive servers, timeout fault handlers, and IPC forwarding in \selfour. First we explore new kernel
objects, followed by new and altered API system calls and invocations, and finally look at structural changes.

\section{Objects}

We add two new objects to the kernel, \emph{\glspl{SCO}} and \emph{resume objects}. Additionally, we modify
the \gls{TCB} object, although do not increase its total size. Finally, we modify the notification
object to allow single-threaded, passive servers to receive signals in
addition to IPC messages.

\subsection{Resume objects}
\label{s:resume}

Resume objects, modelled after KeyKOS~\citep{Bomberger_FFHLS_92}, are a new object type that
generalises the ``reply capabilities'' of baseline \selfour introduced in \cref{p:sel4_ipc}.  Recall that in
baseline \selfour, the receiver of the message (i.e.\ the server) receives the reply capability in a
magic ``reply slot'' in its capability space. The server replies by invoking that capability. Resume
objects remove the magic by explicitly representing the reply channel, and servers with multiple
clients can dedicate a resume object per client.  Instead of generating a one-shot reply capability
in a reply slot on a \call, the operation populates a resume object, while \recv de-populates the
resume object.  Additionally, resume objects also provide more efficient support for stateful servers that handle concurrent
client sessions, which we expand on further when we introduce the changed system-call \gls{API} in
\cref{s:new-api}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{resume}
    \caption{State diagram of resume objects.}
    \label{f:resume-state-diagram}
\end{figure}

A resume object can be in the following three states:
\begin{itemize}
    \item \emph{idle} meaning it is not currently being used,
    \item \emph{waiting} meaning it is being used by a thread blocked and waiting for a message, 
    \item or \emph{active}, which means the resume object currently has a thread blocked waiting for a reply associated with it.
\end{itemize}
Valid state transitions are shown in \cref{f:resume-state-diagram}. Resume objects are now required
as arguments  to
receiving system calls along with endpoint capabilities, which transitions them from idle to
waiting.
If the endpoint is invoked with \call, the caller is blocked on the resume capability, and
it transitions to active.  
If a resume object is directly invoked, using a sending system call (recall from
\cref{sec:sel4-system-call-and-invocations} that sending system calls are \send,
\nbsend, \call, \reply) then a reply message is delivered to the thread blocked on
the object. Finally, if a resume object is in an active state, and provided to \recv, the object
is first invoked, and removed from the call stack, which we now examine in detail. 


\subsubsection{The call stack}

Active resume objects track the \emph{call stack} that is built as nested \call operations take
place. \call triggers a push operation, adding to the top of the stack, while a reply message
triggers a
pop, removing the top of the stack.  The call stack allows us to track the path of a donated
scheduling context, from caller to callee, so that it can be returned to the previous caller,
regardless of which thread sends the reply message. This is a direct consequence of flexible resume
capabilities:  a resume capability can be moved between threads, and \emph{any} thread can execute the
reply: usually the server, but occasionally a timeout fault handler, or a nested server which
received the resume capability via \gls{IPC} forwarding. It is therefore impossible to derive the
original callee from the thread which sent the reply message. When a reply message is sent via an 
active resume object at the head of the call stack, that resume object is popped, and the \gls{SCO} is
linked to the thread that the resume object pointed to, overriding the previous \gls{SCO} to
\gls{TCB} link.

The call stack is structured as a doubly-linked list, with one minor difference: the head of the
call stack is the scheduling context that was donated along the stack, which itself contains a
pointer to the thread currently executing on it. Each resume object then forms a node in the
stack, going back to the original caller at the base. The \gls{SCO} remains the head of the stack
until the \gls{SCO} returns to the initial caller and the stack is fully dismantled.  When a reply
message is sent, the scheduling context travels back along the call stack and the head resume object
is popped.  Reply objects also point to the thread which donated the \gls{SCO} along the stack,
allowing the \gls{SCO} to be returned to that thread when a reply message is sent.  This process is
illustrated in \cref{f:reply-stack}, where $A$ has called $S_{1}$ which has called  $S_{2}$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{replychain}
    \caption[Reply stack example]{The state of the reply stack when a nested server $S_{2}$ is
performing a request on behalf of an initial server $S_{1}$ for client $A$.}
    \label{f:reply-stack}
\end{figure}

In a capability system, one of the biggest challenges is that capabilities can be revoked, and the
object they grant access to deleted, at any time, regardless of the state of the system. Therefore
we provide the following deletion semantics for resume objects:

\begin{itemize}
    \item If the \gls{SCO} is deleted, the head of the call stack becomes the first resume object. To avoid
        a long-running operation, the call stack of resume objects remains linked, and is dismantled
        as each resume object is deleted.
    \item If the head resume object (the object that points to the \gls{SCO}) is deleted, the \gls{SCO} is
        returned along the stack to the caller. 
    \item If a resume object in the middle of the stack is deleted, we use the standard operation 
        for removing
        a node from a doubly-linked list: the previous node is connected to the next node,
        and the deleted object is no longer pointed to by any member of the stack.
    \item If the resume object is at the start of the stack, \ie the node corresponding to the initial
        caller, it is simply removed. The \gls{SCO} cannot return to the initial caller by means of a reply
        message.
\end{itemize}

\begin{table}[t]
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabularx}{\textwidth}{llX}\toprule
        \emph{Field}   & \emph{Type}  & \emph{Description} \\\midrule
        \code{tcb}     & \code{uintptr\_t}                                                                            & The calling or waiting thread that is blocked on this reply object. \\
        \code{prev}    & \code{uintptr\_t}                                                                            & 0 if this is the start of the call stack, otherwise points to the previous
        reply object in the call stack. \\
        \code{next}    & \code{uintptr\_t}                                                                            & Either a pointer to the scheduling context that was last donated using this
        reply object, if this reply object is the head of a call stack (the last caller before the
        server) or a pointer to the next reply object in the stack. 0 if no scheduling context was
        passed along the stack.\\\bottomrule
    \end{tabularx}
    \caption{Fields in a resume object.}
    \label{tab:reply_object}
\end{table}


Resume objects are small (16 bytes on 32-bit platforms, and 32 bytes on 64-bit platforms), and
contain the fields shown in \cref{tab:reply_object}.
   
\subsection{Scheduling context objects}
\label{s:sco}

We introduce a new kernel object type, \glspl{SCO}, which all processing time is accounted against, 
and a new scheduler invariant: any thread in the scheduler queues must have an \gls{SCO}. 
\glspl{SCO} are variable-sized objects that represent access to a certain amount of time and
consist of a core amount of fields, and a circular buffer to track the consumption of time.
Scheduling contexts encapsulate processor time reservations,
derived from sporadic task parameters: minimum inter-arrival time ($T$) and a set of replenishments which is
populated from an original execution budget ($C$), representing the reserved rate
($U$ = $\frac{C}{T}$).
Fields in an \gls{SCO} are shown in \cref{t:sc-fields}.

\begin{table}[b] 
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabularx}{\textwidth}{llX}\toprule
        \emph{Field}   & \emph{Type} & \emph{Description}\\\midrule
        \code{Period}  & \code{uint64\_t} & The replenishment period, $T$. \\
        \code{Consumed} & \code{uint64\_t} & The amount of cycles consumed since the last reset. \\
        \code{Core}     & \code{word\_t} & The ID of the processor this scheduling context grants time on.\\
        \code{TCB}      & \code{uintptr\_t} & A pointer to the thread (if any) that this scheduling context is
        currently providing time to.\\
        \code{Reply}    & \code{uintptr\_t} & A pointer to the head resume object (if any) in a call stack.\\
        \code{Notification} & \code{uintptr\_t} & A pointer to the notification object (if any) to which this \gls{SCO} is bound.\\
        \code{Badge} & \code{word\_t} & An unforgeable identifier for this scheduling context, which is delivered as part of a
        timeout fault message to identify the faulting client.\\
        \code{YieldFrom} & \code{uintptr\_t} & Pointer to a \gls{TCB} that has performed a directed yield to this
        \gls{SCO}.\\
        \code{Head,Tail} & \code{word\_t} & Indexes to the circular buffer of sporadic replenishments.\\
        \code{Max} & \code{word\_t} & Size of the circular buffer of replenishments for this scheduling context.\\
        \bottomrule
    \end{tabularx}
    \caption{Fields of a scheduling context object.}
    \label{t:sc-fields}
\end{table}

\subsubsection{Replenishments}

In addition to the core fields, \glspl{SCO} contain a variable amount of \emph{replenishments},
which consist of a \code{timestamp} and \code{amount}. These are used for both round-robin and
sporadic threads. For round-robin threads we simply use the head replenishment to track how much
time is left in that \gls{SCO}'s timeslice. 

Sporadic threads are more complex; the replenishments form a circular buffer used to track 
how much time a thread can execute for (\code{amount}) and when that time can be used (\code{timestamp}).
The size of the circular buffer is limited by both a variable
provided on configuration of the \gls{SCO}, and the size of the \gls{SCO} (where the former must be
$\leq$ the latter). This allows system
designers to control the preemption and fragmentation of sporadic replenishments as discussed in
\cref{sec:model-sporadic}. 

Each \gls{SCO} is minimum $2^{8}$ bytes in size, which can hold eight or ten replenishments for 32
and 64-bit processors respectively. This is sufficient for most uses, but more replenishments can
be supported by larger sizes, allowing system designers to trade-off latency for fragmentation
overhead in the sporadic server implementation. \glspl{SCO} can be created as larger
objects, $2^{n}$ bytes, then the rest of the object can be filled with replenishments, as
shown in \cref{t:impl-sc-layout}. System designers can then use the \code{max} replenishment field
to specify the exact number of replenishment to use, up to the object size. 

\begin{table}[t] 
    \centering
    \begin{tabular}{c|c|c|c|c|}\cline{2-5}
        0x00 &  \multicolumn{2}{c}{\texttt{period}} & \multicolumn{2}{|c|}{\texttt{consumed}}
        \\\cline{2-5}
        0x10 & \texttt{core}                         & \texttt{TCB} & \texttt{reply} & \texttt{ntfn} \\\cline{2-5}
        0x20 &\texttt{badge}                        & \texttt{yieldFrom}                               & \texttt{head}  & \texttt{tail} \\\cline{2-5}
        0x30 & \texttt{max}                          &                                                  &                & \\\cline{2-5}
        0x40 & \multicolumn{4}{c|}{\texttt{replenishment$_{0}$}}  \\\cline{2-5}
        0x50 & \multicolumn{4}{c|}{\texttt{replenishment$_{1}$}}  \\\cline{2-5}
        \ldots & \multicolumn{4}{c|}{\ldots}  \\\cline{2-5}
        0xF0 & \multicolumn{4}{c|}{\texttt{replenishment$_{n}$}}  \\\cline{2-5}


    \end{tabular}
    \caption[Layout of a scheduling context object.]{Layout of a scheduling context object on a 32-bit system.}
    \label{t:impl-sc-layout}
\end{table}

\subsubsection{Admission}

Like any \selfour object, scheduling contexts are created from zeroed, untyped memory. Consequently,
new scheduling objects do not grant authority to any processing time at all, as the parameters are
all set to zero. To configure a scheduling context, the new \schedcontrol capability must be
invoked, which allows the period, initial sporadic replenishment, maximum number of refills, and
badge fields to be set. The processing core is derived from the \schedcontrol capability
that is invoked: only one exists per core. 
The \schedcontrolconfigure operation behaves differently depending on the state of the target scheduling
context, and can be used not only to configure \glspl{SCO} but also to migrate threads across cores, and
change the available bandwidth of a currently runnable thread. 
\clearpage
Semantics are as follows:

\begin{listing}[t]
    \inputminted{c}{code/refill_new.c}
    \caption[Refill new routine.]{\code{refill\_new} routine to initialise a scheduling context that is not active or is
    migrating cores. \code{HEAD} is short-hand for access the head of the circular buffer of
    replenishments.}
    \label{list:refill-new}
    \inputminted{c}{code/refill_update.c}
    \caption[Refill update routine.]{\code{refill\_update} routine to change the parameters of an
            active scheduling context. \code{INDEX}, \code{TAIL}, and \code{NEXT} are operations on 
            the circular buffer.}
\label{list:refill-update}
\end{listing}

\begin{itemize}
\item If the scheduling context is not bound
to a currently runnable thread, or is empty---in that the parameters are set to 0---the operation is 
a basic configure: the fields are
simply set and the first replenishment is configured with the budget provided and the timestamp of the
kernel entry (see \cref{list:refill-new}).
\item If the scheduling context is bound to a currently runnable thread, but that scheduling
context is for a different processing core than the \schedcontrol capability, the fields are
set and the thread is migrated to the new core.  
\item Finally, if the scheduling context is bound to a currently running thread and the
    \schedcontrol capability is for the same core, we update the replenishment list without
    allowing the sliding window constraint to be violated. \cref{list:refill-update} shows the
    algorithm used.
\end{itemize}

\begin{listing}
\inputminted{c}{code/refill_used.c}
\caption[Schedule used routine.]{\code{schedule\_used} routine, used below.}
\inputminted{c}{code/refill_check.c}
\caption[Check budget routine.]{\code{check\_budget} routine used to implement sporadic servers.}
\label{list:check}
\end{listing}

\subsubsection{Sporadic Servers}
\label{sec:impl-sporadic}

Recall that polling servers (\cref{p:polling-servers}) provide a specified bandwidth to a thread,
but once a thread wakes, if it blocks or is preempted, the budget is abandoned until the next
period. Sporadic servers (\cref{p:sporadic}) also limit threads to a specified execution bandwidth,
but do so by tracking a buffer of replenishments, which is appended to each time a thread is
preempted or blocks. Sporadic servers with only one replenishment act like polling servers.
As a result, configured scheduling contexts with zero extra replenishments behave like polling servers,
otherwise they behave as sporadic servers,  allowing application developers to
tune the behaviour of threads depending on their preemption levels and execution durations.

\begin{listing}
\inputminted{c}{code/split_check.c}
\caption[Split check routine.]{\code{split\_check} routine used to implement sporadic servers.}
\label{list:split}
\end{listing}

The algorithms for managing replenishments are taken from \citet{Danish_LW_11}, with adjustments to
support periods of 0 (for round robin threads) and to implement a minimum budget.  Whenever the
current scheduling context is changed, \code{check\_budget} (\cref{list:check}) is called to bill the amount of time consumed since the last scheduling
context change. If the budget is not completely consumed by \code{check\_budget},
\code{split\_budget} (\cref{list:split}) is called to schedule the
subsequent refill for the chunk of time just consumed.  If the replenishment buffer is full, or the
amount consumed is less than the minimum budget, the amount used is merged into the next
replenishment.  When a new scheduling context is switched to, \code{unblock\_check} 
(\cref{list:unblock-check}) is used, which merges any replenishments that are already available,
avoiding unnecessary preemptions.

\begin{listing}[t!]
\inputminted{c}{code/split_check.c}
\caption[Unblock check routine.]{\code{unblock\_check} routine used to implement sporadic servers.}
\label{list:unblock-check}
\end{listing}

The full code for sporadic servers, with less brevity than the simplified samples included here, is
available in \cref{sporadic-code}.

\subsection{Thread control blocks}

Recall from \cref{sec:sel4-tcb} that \glspl{TCB} are the abstraction of an execution context in
\selfour, which are formed from a TCB data structure and a special \cnode containing capabilities
specific to that thread, which is only accessible to the kernel. We make several alterations to this
structure, but do not impact the \gls{TCB} size, as there was enough space available. The altered
fields are shown in \cref{t:tcb-fields}.

\begin{table}[t] 
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabularx}{\textwidth}{llX}\toprule
        \emph{Field}              & \emph{Type}           & \emph{Description}\\\midrule
    \sout{\code{timeslice}}       & \sout{\code{word\_t}} & Used to track the timeslice, replaced by the scheduling context. \\
        \code{scheduling context} & \code{uintptr\_t}     & The scheduling context this \gls{TCB}
        consumes time from, 0 if this is a passive \gls{TCB}. \\
        \code{MCP}   & \code{word\_t}    & The \gls{MCP} of this \gls{TCB}. \\
        \code{reply} & \code{uintptr\_t} & A pointer to the resume object this TCB is blocked on, if the TCB is
        \code{BlockedOnReply} or \code{BlockedOnRecv}. \\
        \code{yieldTo} & \code{uintptr\_t} & Pointer to the \gls{SCO} this \gls{TCB} has performed a directed yield to,
        if any.\\
        \sout{\code{faultEndpoint}} & \sout{\code{word\_t}} & Moved to the TCB \cnode. \\
        \bottomrule
    \end{tabularx}
    \caption[Added and removed fields in a TCB.]{Added and removed fields in a \gls{TCB} object.}
    \label{t:tcb-fields}
\end{table}


\subsubsection{Fault endpoints}

We also change the contents of the TCB \cnode, removing two slots previously required by the reply
capability implementation, as resume objects are now provided to receiving system calls, and
no slot in the TCB \cnode is required. In addition, we add two new slots by installing fault handler
capabilities, for faults and timeouts, into the TCB \cnode. This is an optimisation for fault
handling capabilities: in baseline seL4, the fault-handling capability is looked up on every single fault, increasing the overhead of
fault handling. To minimise this overhead, we look up the fault endpoint when it is configured, and
copy the capability into the TCB \cnode.
Note that capabilities in the TCB \cnode cannot be changed
by user-level: the TCB itself must be deleted for the endpoint capability to be fully revoked and
the object deleted, which avoids a time-of-check time-of-use vulnerability.

\subsubsection{Maximum controlled priorities}

The MCP, or \emph{maximum controlled priority}, resurrects a concept from early L4
kernels~\citep{Liedtke_96:rm}. It supports lightweight, limited manipulation of thread priorities,
useful e.g.\ for implementing user-level thread packages. When setting
the priority or MCP of a TCB, \(A\),
the caller must provide the capability to a TCB, \(B\), (which could be the caller's
TCB). The caller is allowed to set the priority or MCP of \(A\) up to the value
of \(B\)'s MCP.\footnote{Obviously, this operation also requires a capability to \(A\)'s TCB.}
In a typical system, most threads
will run with an MCP of zero and have no access to TCB capabilities with a higher MCP, meaning they
cannot raise any thread's priority.
The MCP is taken from an explicitly-provided TCB, rather than the caller's, to avoid the
confused deputy problem~\citep{Hardy_88}.

\subsection{Notification objects}

A scheduling context object can be associated with a notification object, which allows a passive
server with bound notifications to receive signals and run on the notification's
scheduling context to process those signals.
We add a pointer to the scheduling context from the notification context, as well as vice-versa, to 
facilitate this mechanism, which increases the size of the notification object from $2^{4}$ to
$2^{5}$ on 32-bit, and $2^{5}$ to $2^{6}$ on 64-bit. 

\section{MCS API}
\label{s:new-api}

We now present the new \gls{MCS} API for \selfour, which alters the core system call API, 
as well as adding new invocations and modifying existing ones. In this section we also
present the semantics of scheduling context donation, which are directly linked to the new API.

\subsection{Waiting system calls}

In \cref{sec:sel4-system-call-and-invocations} we divided the \selfour system call API into two
classes: sending system calls (\send, \nbsend, \call, \reply),
receiving system calls (\recv, \nbrecv), plus combinations of both for fast 
RPC (\call, \replyrecv).  

Our implementation alters the meaning of a receiving system call, and adds a new class of system
call: \emph{waiting} system calls. The difference is simple: receiving system calls are expected to
provide a capability to a resume object, which can be used if a \call is received over
the endpoint being blocked on. Waiting system calls do not, and cannot be paired with a \call
successfully. Only receiving system calls can be used with scheduling context donation, as the reply
object is used to track the call stack. Such a distinction existed in the original L4 model, where
and were referred to as open and closed receive.

Additionally, because the TCB reply capability slot is dropped the \gls{TCB} \cnode, we remove the \reply 
system call, as its only purpose was to invoke the capability in the reply slot, which no longer exists.
\replyrecv remains, and invokes the resume capability, sending the reply, before using the
reply in the \recv phase. 

By making the reply channel explicit, we significantly increase the practicality of the IPC
fastpath. In baseline \selfour, any server that served multiple clients and did not reply
immediately---because it needed to block on I/O, for example---would save the reply
capability, moving it from the special TCB slot to a standard slot in the \cspace. Later, when
the server replied to the client, it would invoke the saved reply capability with \send, then block
for new messages with \recv, avoiding the IPC fastpath for \replyrecv. No system call to move the 
resume capability back to the \gls{TCB} \cnode was provided, however this would require yet another
system call. Resume objects avoid the save
slowpath, and increase the probability of using the fastpath, should the conditions detailed in \cref{sec:sel4-fastpath} be satisfied.

\subsection{IPC Forwarding}
\label{s:ipc-forwarding}

For sending system calls, only the system calls that combine a send- and receive-phase can donate
a scheduling context, as a thread must be blocked in order to receive a scheduling context. Both
\call and \replyrecv combine a \send and \recv phase, although with slightly different semantics
with respect to resume objects. However, \call conducts both phases on the same capability, while
the send phase of \replyrecv can only act on a resume object. We introduce a new combined system
call, \nbsendrecv, which is the mechanism for IPC forwarding (\cref{sec:ipc-forwarding}). 
IPC forwarding allows a \nbsend followed by a \recv on two distinct capabilities, without the
restriction that the send-phase must act on a resume capability. 
Additionally, we provide a variant which combines a send- and a waiting-phase, \nbsendwait.
Both variants allow for IPC forwarding, and can donate a scheduling context
along with the IPC, by-passing the call chain. 

The first phase of \nbsendrecv and \nbsendwait must use an \nbsend rather than a \send
to avoid introducing a long-running operation in the kernel in the form of a chain reaction
triggered by a single thread blocking. We demonstrate by example why the send-phase must be
non-blocking.

\begin{figure}
    \includegraphics[width=\textwidth]{nbsendwait}
    \caption{The problem with \sendrecv.}
    \label{fig:send-wait}
\end{figure}

To illustrate the problem, we refer to \cref{fig:send-wait}, which shows five threads, $T_{z}$ and
$T_{1},\ldots,T_{4}$ and four endpoints, $E_{1},\dots,E_{4}$. Black arrows have already
occurred, blocking each thread on an endpoint, and grey are pending once each. $T_{1}$ has used \sendrecv on
$E_{1}$ and $E_{2}$, $T_{2}$ has used \sendrecv on $E_{2}$ and $E_{3}$,  
$T_{3}$ has used \sendrecv on $E_{3}$ and $E_{4}$. $T_{4}$ is blocked sending on $E_{4}$,
however the chain could continue without bound.
Because the send-phase is blocking, each thread $T_{n}$ is blocked waiting for the send to proceed 
on $E_{n}$. $T_{z}$ then uses \recv on $E_{1}$ (the red arrow), which triggers an unbounded chain of message
sending: $T_{1}$ finishes its send phase and begins the receive phase on $E_{2}$, which allows
$T_{2}$ to finish its send phase and begin the receive phase on $E_{3}$, which allows $T_{3}$ to
finish its send phase and start the receive phase on $E_{4}$. This chain reaction is an
unbounded, long-running operation. By making the send-phase non-blocking such a chain reaction is not
possible: the send-phase is aborted as the endpoint has no threads waiting for a message on it, and
the receive phase then proceeds. 

\subsubsection{Yield}

In baseline \selfour, \yield triggers a reschedule, and appends the calling thread to the end of the
appropriate scheduling queue. We retain these semantics for full scheduling contexts, however 
for partial, sporadic scheduling contexts \yield depletes the head sporadic replenishment immediately. The
caller is then blocked until the next replenishment becomes available, and then placed at the end of
the appropriate scheduling queue.

We also provide a new invocation on scheduling context capabilities, \scyieldto, which allows for a directed
yield to a specific \gls{SCO}. When called on a \gls{SCO} with a \gls{TCB} running at the same
priority as the caller, \scyieldto results in that thread being switched to by the kernel scheduler.
Otherwise the \gls{TCB} is moved to the head of the scheduling queue for its priority.

\begin{table}[t]
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabularx}{\textwidth}{lXlll}\toprule
        \emph{System Call} & \emph{Class}       & \emph{Donation?} & \emph{New?} & \emph{Definition} \\\midrule
        \call              & sending, receiving & \yes             & \no         & \cref{api:call}\\
        \send              & sending            & \no              & \no         & \cref{api:send} \\
        \nbsend            & sending            & \no              & \no         & \cref{api:nbsend} \\
        \recv              & receiving          & \yes             & \no         & \cref{api:recv} \\
        \nbrecv            & receiving          & \yes             & \no         & \cref{api:nbrecv} \\
        \wait              & waiting            & \no              & \yes        & \cref{api:wait}\\
        \nbwait            & waiting            & \no              & \yes        & \cref{api:nbwait} \\
        \replyrecv         & sending, receiving & \yes             & \no         & \cref{api:replyrecv} \\
        \nbsendrecv        & sending, receiving & \yes             & \yes        & \cref{api:nbsendrecv} \\
        \nbsendwait        & sending, waiting   & \no              & \yes        & \cref{api:nbsendwait} \\
        \yield             & scheduling         & \no              & \no         & \cref{api:yield} \\
        \sout{\reply}      & \sout{sending}     & -                & -           & Removed \\
    \end{tabularx}
    \caption[New seL4 system call summary.]{New \selfour system call summary, indicating which system calls can trigger donation and/or receiving messages. }
    \label{t:new-system-calls}
\end{table}

\cref{t:new-system-calls} summarises the new MCS system call API, while more detailed 
descriptions of each system call can be found in \cref{appendix:api}.

\subsection{Invocations}

Scheduling contexts have five invocations, listed in \cref{tab:sched_context_api}. Three of those
invocations are for binding \glspl{SCO} to \gls{TCB} and notification objects, while \scyieldto and 
\scconsumed facilitate user-level scheduling, allowing the user to manipulate
the kernel scheduling queues and to query the amount of time consumed by a specific \gls{SCO}.
Both functions return the amount of CPU time the scheduling context has consumed since it was last
cleared: reading the consumed value by either system call clears it, as does a timeout
exception.

The new control
capability, \schedcontrol, has only one invocation, \schedcontrolconfigure, for setting the parameters of a
scheduling context (the function definition can be found in \cref{api:schedcontrol_configure}). 
    
\begin{table}
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabularx}{\textwidth}{lXl} \toprule
        \emph{Invocation} & \emph{Description} & \emph{Definition} \\\midrule
        \scbind    & Bind an object (TCB or Notification) to an \gls{SCO}. & \cref{api:schedcontext_bind} \\
        \scunbind  & Unbind all objects from an \gls{SCO}. & \cref{api:schedcontext_unbind} \\
        \scunbindobject & Unbind a specific object from an \gls{SCO}. & \cref{api:schedcontext_unbind_object}\\
        \scconsumed & Return the amount of time since the last timeout fault, \scconsumed or
        \scyieldto was called. & \cref{api:schedcontext_consumed}\\ 
        \scyieldto  & Place the thread bound to this \gls{SCO} at the front of its priority queue and return
        any time consumed. & \cref{api:schedcontext_yieldto}\\
        \bottomrule
    \end{tabularx}
    \caption{Scheduling context capability invocations.}
    \label{tab:sched_context_api}
\end{table}

As baseline \selfour only supports sending three capabilities in a single message (including to the
kernel), we add a second method for configuring multiple fields on a TCB in one system call
(\tcbsetschedparams), in addition to the existing method (\tcbconfigure). We avoid altering the
existing limit to avoid extensive re-verification of a non-critical path. Full configuration
of a TCB requires up to six capabilities to be set, not including timeout handlers, which are not
included in a combined configuration call as this is a less common operation. 
In total, we alter two invocations on TCB objects
and add three new invocations, shown in \cref{tab:tcb_api}. 

\begin{table}
    \centering
    \rowcolors{2}{gray!25}{}
    \begin{tabularx}{\textwidth}{lXll} \toprule
        \emph{Invocation} & \emph{Description} & \emph{New?} & \emph{Definition} \\\midrule
        \tcbconfigure & Set the cnode, vspace, and \gls{IPC} buffer. & \no & \cref{api:tcb_configure}\\
        \tcbsetmcpriority & Set the \gls{MCP}. & \yes & \cref{api:tcb_setmcpriority} \\
        \tcbsetpriority & Set the priority. & \no & \cref{api:tcb_setpriority} \\
        \tcbsetschedparams & Set \gls{SCO}, \gls{MCP}, priority and fault endpoint. & \yes &
        \cref{api:tcb_setschedparams} \\
        \tcbsettimeoutep & Set the timeout endpoint. & \yes & \cref{api:tcb_settimeoutep} \\
        \sout{\tcbsetaffinity}                 & Removed, now derived from \gls{SCO}.  & \no & N/A \\
        \bottomrule
    \end{tabularx}
    \caption{New and altered \gls{TCB} capability invocations.}
    \label{tab:tcb_api}
\end{table}

Finally, we remove the \cnodesavecaller invocation, which was previously used to move a reply
capability from the \gls{TCB} \cnode to a slot in the \gls{TCB} \cspace, and is no longer required
as the resume object capability is now provided to the receiving system call. 

\subsection{Scheduling context donation}

Conceptually there are three types of scheduling context donation: lending, returning, and
forwarding a scheduling context. The type of donation that occurs, and if it can occur, depends on
the system call used. 

Lending a scheduling context occurs between caller and callee on a \call, and
the semantics are simple: if a thread 
blocked on an endpoint does not have a scheduling context, the scheduling context is transferred
from sender (caller) to receiver (the callee), and the resume object is pushed
onto the call-stack, such that the \gls{SCO} can be returned. If the callee already has a scheduling
context, scheduling context donation does not occur. Note that lending also works for
handling faults, so thread fault handlers can be passive. Unlike previous versions of timeslice
donation in L4 kernels, there is no explicit flag for permitting donation: if the callee does not
have a scheduling context, the caller would block indefinitely, as the callee has no other way to
access processing time. As discussed in \cref{s:locking},
the caller in an \gls{RPC}-style operation must trust the callee: regardless of the scheduling context
being executed upon, the callee can always choose to not reply, blocking the caller indefinitely. 

Lent scheduling contexts are returned when the resume object is invoked, or the head resume object
is deleted. Resume objects 
can be invoked directly by the send phase of a system call, or indirectly, by using the
resume object in the receive phase of a system call, thereby clearing it for another client to block
on, or deleting the resume object.
Regardless of the state of the callee, the \gls{SCO} is returned, which if done
incorrectly (via a \send), can leave the callee not blocked on an endpoint, and consequently 
unable to receive further scheduling contexts via donation or lending. Correct usage by a passive thread is with \replyrecv,
\nbsendrecv, blocking the passive thread such that it is ready to receive another scheduling
context. Like lending, scheduling contexts can also be returned by a fault handler replying to a fault message.

Forwarding a scheduling context does not establish a call-chain relationship, and allows
for scheduling contexts to be transferred between threads in non-\gls{RPC} patterns. The semantics are
as follows: if \gls{IPC} forwarding is used via \nbsendrecv (\cref{s:ipc-forwarding}) is used, and the receiver does not have a scheduling context,
the scheduling context is forwarded. 

Scheduling contexts can only be donated and received on specific system calls, indicated in
\cref{t:new-system-calls}. 

\subsubsection{Passive servers}
\label{sec:impl-passive-servers}

Passive servers do not have scheduling contexts, yet they must be waiting on an endpoint in order to
receive \glspl{SCO} over \gls{IPC}, and possibly initialise some server-specific state first.
The protocol for initialising passive servers is to start them as
active, then wait for the server to signal to the initialising thread that they are ready to be
converted to passive. IPC forwarding can be used to do this, so the passive server can send a
message on one endpoint or notification object, then wait on another to receive \glspl{SCO} over
IPC. Example user-level code for this process is 
provided in \cref{list:passive-server}. 

\begin{listing}[t]
\begin{minted}{c}
void initialiser(seL4_CPtr server_tcb, seL4_CPtr init_sc, seL4_CPtr init_endpoint) {
  /* start server */
  seL4_TCB_Resume(server_tcb);
  /* wait for the server to tell us it is initialised */
  seL4_Wait(init_endpoint, NULL);
  /* convert the server to passive */
  seL4_SchedContext_Unbind(init_sc);
}

void passive_server(seL4_CPtr init_endpoint, seL4_CPtr endpoint, seL4_CPtr reply) {  
  seL4_MessageInfo_t info = init_server_state();
  seL4_Word badge;

  /* signal to the initialiser that this server is ready to be converted to passive, and block on the endpoint with resume object reply */
  info = seL4_NBSendRecv(init_endpoint, info, endpoint, &badge, reply);
  while (true) {
    /* when the server wakes, it is running on a client scheduling context */
    info = process_request(sender);
    /* reply to the client and block on endpoint, with resume object reply */
    seL4_ReplyRecv(endpoint, info, &badge, reply);
  }
}

void client(seL4_CPtr endpoint, seL4_MessageInfo_t message) {
  /* send a message to the passive server */
  seL4_Call(endpoint, message);
}
\end{minted}
\caption{Example initialiser, passive server, and client.}
\label{list:passive-server}
\end{listing}

\subsubsection{Notification Binding}

Passive servers can receive scheduling contexts from their bound notification object, allowing for
the construction of single-threaded servers which receive both notifications and IPC messages. 
The semantics are as follows: if a TCB receives a notification from its bound notification
object, and that TCB does not have a scheduling context, the TCB receives the scheduling context.
If a TCB blocks on an endpoint, and is running on its bound notification object, the TCB is
rendered passive again. 

\section{Data Structures and Algorithms}

Now we discuss changes to the data structures and algorithms added and changed in the kernel to provide
accounting, which charges CPU time to the correct \gls{SCO}, and the enforcement mechanisms, which requires
a new scheduling data structure and fault type.

\subsection{Accounting}

All processing time is accounted to the scheduling context of the currently running \gls{TCB}. 
In this section we first establish \emph{how} that time is accounted, and then \emph{when} it is
accounted.

There are two ways to account for time in an \gls{OS} kernel:
\begin{itemize}
    \item in fixed time quanta, referred to as \emph{ticks},
    \item in actual time passed between events, referred to as \emph{tickless}.
\end{itemize}

Using ticks, timer interrupts are set for a periodic tick and are
handled even if no kernel operation is required, incurring a preemption overhead.
This approach has the advantage of simplicity: the timer implementation in the kernel is 
stateless and the timer driver can be set once, periodically, never requiring reprogramming. 
In older-generation hardware, especially x86, reprogramming the timer device incurred a 
significant cost, enough to ameliorate the preemption overhead. 
However, this design is not without limitations:
the precision of the scheduler is reduced to the length of the tick. Precision must be traded for
preemption overhead: reducing the tick length increases precision, but also increases 
preemption overhead. Preemption overhead is particularly problematic for real-time systems, as
the \gls{WCET} each real-time task is inflated by the \gls{WCET} of the kernel for every preemption.

Tickless kernels remove this trade-off by setting timer interrupts for the exact time of the next
event. As we will show in \cref{s:eval-timer}, the cost of reprogramming the timer device on modern
hardware is smaller, rendering the tickless design feasible. Consequently, we convert \selfour
to a tickless kernel, a non-trivial change due to the fact that the kernel is
non-preemptible\footnote{Save for
explicit preemption points in a few long-running operations.}, which means timer interrupts for the
scheduler can only be serviced when the kernel is not running. Allowing a thread to complete an
operation in the kernel when it does not have sufficient time to do so would violate temporal
isolation, as the bandwidth allocated to the scheduling context could then be exceeded. As a result,  
on kernel entry, the calling thread must have sufficient budget to complete the operation. Without
further decoding a pending system call, the kernel cannot tell which operation a thread is
attempting, so sufficient budget becomes the \gls{WCET} of the kernel.

Threads are not permitted in the scheduler if they have insufficient budget, as if the scheduling
algorithm must iterate until a thread with sufficient budget is discovered, the complexity of the
algorithm becomes $O(n)$ in the number of threads. The same condition holds for IPC endpoint queues:
we take the time on kernel entry in order to charge a preempting thread, and any time from kernel
entry is accounted to the scheduling context active at kernel exit. Because a passive scheduling context
then must be able to be charged not only for the entry path into the kernel, but the exit path, the
sufficient budget is actually \emph{twice} the \gls{WCET} of the kernel.

We check budget sufficiency on kernel entry, by reading the current timestamp and storing 
the amount of time consumed since the last kernel entry, allowing us to detect budget expiry in a single 
point, and carry on other kernel operations with this assumption intact. This means that timeout 
faults can only be raised at one point in the kernel, greatly simplifying the implementation by
avoiding excessive checks in other paths. 

\begin{figure}
    \centering 
    \includegraphics[width=0.7\textwidth]{tickless}
    \caption{New tickless kernel structure.}
    \label{figure:tickless}
\end{figure}


\subsubsection{Fastpath}
\label{p:impl-fastpath}

We alter the IPC fastpath conditions introduced in \cref{sec:sel4-fastpath} to include one
more condition to have a very low overhead on fastpath performance:
\begin{itemize}
    \item The receiver must be passive. For \call, this means the callee must be passive, and
          for \replyrecv, the TCB blocked on the resume object must be passive.
\end{itemize}
This is because
although timer device access is cheaper on modern hardware, it is not free. It also allows us to
avoid checking budgets and modifying sporadic replenishments on the fastpath, operations which have
many conditional branches, which would also increase overheads. Note that we expect most servers in
real systems to use passive servers for \gls{RPC}, even for non real-time systems, to assure
fairness of resource access between round-robin threads.  

Even on the slowpath, we avoid reprogramming the timer unless it is required. Like reading the time, 
reprogramming the timer is cheaper, but not free. Consequently although we update the current time
on every entry, we only charge threads at specific points:
\begin{itemize}
\item when the timer interrupt has fired, so clearly the timer needs reprogramming,
\item when the current scheduling context changes, so a different interrupt is required,
\item when the kernel timestamp has been acted upon: for instance, used to schedule a
    replenishment.
\end{itemize}
If the recorded timestamp is not acted upon, the time is rolled back to the previously recorded value,
thus avoiding reprogramming the timer unnecessarily.

\cref{figure:tickless} illustrates the new control flow of the tickless kernel, and shows
how and when processing time is charged to scheduling contexts.
On kernel entry, if the available budget is insufficient, the kernel pretends the timer has already fired,
resets the budget and adds the thread to the release queue. If the entry was due to a system call,
the thread will retry that call once it wakes with further budget.
Once the thread is awoken, it will retry the system call.

\subsubsection{Domain scheduler}

We retain the domain scheduler which, as discussed in \cref{sec:sel4-domain-scheduler}, is required
for the proof of confidentiality~\citep{Murray_MBGBSLGK_13}. However, we convert the 
implementation to tickless, such that domains are
configured with CPU cycles to execute, rather than fixed ticks, which no longer exist.

\subsection{Enforcement}

% timeout exceptions, release queue, sporadic servers
The enforcement mechanisms require the most significant changes to the kernel, which we now detail.
The main change required to the existing scheduler is the addition of a \emph{release queue} per
processing core which contains all runnable threads waiting for a replenishment. If a
preempted thread does not have any available replenishments, the kernel removes the thread from the
ready queue.

\subsubsection{Priority queues}

In addition to the release queue, we also change endpoint- and notification-object queues
to priority queues, ordered by \gls{TCB} priority. All queues are implemented as ordered, doubly-linked
lists, where all operations complete in $O(1)$ except for insertion, which is $O(n)$, where $n$ 
is the number of threads in the queue. 
We chose the list data structure over a heap for increased performance and reduced verification
burden. 

A list-based priority queue out-performs a heap-based priority queue for small $n$ (in our
implementation up to around $n = 100$).  This $n$ is larger than one would expect in a traditional
\gls{OS}, where heap implementations are array-based in contiguous memory with layouts optimised for
cache usage.  However, because \selfour kernel memory is
managed at user-level (as discussed in \cref{sec:sel4-memory}), data structures must be dynamic, 
resulting in much higher code complexity, and poorer performance than a static heap. 
For endpoint queues, it is reasonable to expect a low number of threads, 
as in well-designed systems the amount of kernel threads should be minimal to maintain a low memory
profile (note that for systems requiring many threads, user-level or green threads can easily be
deployed as a light-weight solution). 
For the release queue, the restriction
becomes a function of the user-level admission test: only scheduling contexts with 
partial budgets are placed into the release queue, which limits the size of the queue.
Consequently we do not expect any of the priority queues to contain large numbers of threads,
however if scalability becomes a problem in
practice this implementation could be reconsidered. \citet{Brandenburg:phd} used
binomial heaps for schedulers in \litmus, however Linux does not provide the guarantees, or
implementation requirements of \selfour.

The list-based queues are non-preemptible, such that the \gls{WCET} of the kernel is a function of the 
number of threads that can wait on an endpoint.
One security concern is sub-systems with access to a single scheduling context and a large amount
of memory can increase the kernel's run-time by queuing up a large amount of passive threads on 
an endpoint. To prevent this, system designers must not hand out large untyped capabilities to
untrusted sub-systems.

\subsubsection{Timeout Exception Handlers}

Timeout exceptions are only triggered for threads with non-empty timeout handling slots in the
\gls{TCB} \cnode, configured with \tcbsettimeoutep. The implementation is the same as for
fault handlers, with one exception: while fault \gls{IPC} messages can trigger scheduling context
donation, allowing fault-handling threads to be passive, timeout messages cannot. The timeout
message contains the badge of the scheduling context which triggered the fault, and the amount of
time consumed by that scheduling context since the last reset.

\section{Summary}

In this chapter we have presented the new kernel objects, system call API, invocations and 
structural changes made to provide mechanisms for temporal isolation. 
The implementation adds roughly 2,000 lines of code to \selfour (14\% increase), as measured
by source lines of code (SLOC)~\citep{Wheeler_01} on the pre-processed code for the \textsc{Sabre},
which is the verified platform for \selfour.
In the next chapter, we evaluate our implementation with a series of microbenchmarks, 
system benchmarks and case studies.


