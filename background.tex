

% real time models

%% cover basic RT scheduling
%%% EDF
%%% FP
%%% schedulability tests

% operating systems
%% resource kernels
%% microkernels

\chapter{Core concepts}
\label{chap:background}

In this section we provide the background required to motivate and understand our research.
We introduce real-time theory, including scheduling algorithms and resource sharing protocols.
In addition, we define operating systems, microkernels and introduce the concept of a resource kernel.
This thesis draws on all of these fields in order to support real-time and mixed-criticality systems.

\section{Real-time theory}
\label{sec:real-time-theory}

Real-time systems have timing constraints, where the correctness of the system is dependent not only
on the results of computations, but on the time at which those results arrive~\citep{Stankovic_88}.
Essentially all software consumes time: if the software never gets access to processing time, no
results will ever be obtained.  However, for a system to be considered real-time it must be
sensitive to timing behaviour---how much processing time is allocated and when it is allocated. For
most software, time is fungible: it does not matter when the software is run, as long as it does get
run. For real-time software this is not the case.  Consider the case of software that deploys the
brakes on a train: the routine must run in time for the train to stop at the platform. Or, in a more
critical scenario, software that deploys the landing gear on a plane: it must run and the gear must
be fully down before the plane hits the runway.

There are many ways to model real-time systems, which we touch on in the coming chapters. 

\subsection{Types of real-time tasks}

The term \emph{task} in real-time theory is a high-level abstraction used to refer to a logical
sequence of events, which in 
operating systems terms can be realised as a single thread. How tasks
are realised by an operating system---with or without memory protection, for example---is specific to the
implementation.  Computations by tasks in real-time systems have deadlines which determine the
correctness of the system. How those deadlines effect correctness depends on the category of the
system, which is generally referred to as \gls{HRT}, \gls{SRT}, or \emph{best-effort}.

\emph{Hard Real-Time} tasks have deadlines; if a deadline is missed,
the task is considered incorrect. \gls{HRT} tasks are most common in safety-critical systems, where
deadline misses can lead to catastrophic results.  Examples include airbag systems
in cars and control systems for autonomous vehicles.  In the former, missing a deadline could cause
an airbag to be deployed too late.  In the latter, the autonomous vehicle could crash.  To guarantee
that a \gls{HRT} task can meet deadlines, the code must be subject to {\gls{WCET}} analysis. State
of the art {\gls{WCET}} analysis is generally pessimistic by several orders of magnitude on modern
hardware platforms with deep pipelines, complex cache-hierarchies, branch predictors and
memory-management units~\citep{Wilhelm_EEHTWBFHMMPPSS_08}, which
means that allocated time will not generally be used completely, although it must be available.
WCET is pessimistic for multiple reasons: firstly, much behaviour like interrupt arrival times, 
cannot be predicted but only
bounded, so the schedulability analysis incorporated into the WCET includes the worst possible execution case. Secondly, accurate models of modern
hardware are few, so often the WCET must assume caches are not primed and possibly conflicting. 
Further detail about {\gls{WCET}} analysis can be found in \citet{Lv_GZDYZ_09}.

\emph{Soft Real-Time} tasks, as opposed to \gls{HRT}, can be
considered correct in the presence of some defined level of deadline misses. Although {\gls{WCET}}
can be known for {\gls{SRT}} tasks, less precise but more optimistic time estimates are generally
used for practicality.  Examples of \gls{SRT} tasks can be found in multimedia and video game platforms,
where deadline misses may result in some non-critical effect such as performance degradation.
\gls{SRT} tasks can also be found in safety-critical systems where the result degrades if not enough
time has been
allocated by the deadline, but the result remains useful \eg image recognition and object detection
in autonomous vehicles. In such tasks, a minimum allocation of time to the task might be \gls{HRT},
while further allocations are \gls{SRT}.

Various models exist to quantify permissible deadline misses in \gls{SRT} systems.  One measure is
to consider the upper bound of how much the deadline is missed, referred to as
\emph{tardiness}~\citep{Devi:phd}.  Another is to express an upper bound on the percentage of
deadline misses permitted for the system to be considered correct, which is easier to measure but
less meaningful. Some \gls{SRT} systems allow deadlines to be skipped
completely~\citep{Koren_Shasha_95} while others allow deadlines to be postponed. Systems that allow
deadlines to be skipped are often referred to as firm real-time \eg media processing, where some
frames can be dropped. 

\emph{Best-effort} tasks do not have temporal requirements, and generally execute in the
background of a real-time system. Examples include logging services and standard applications, but
may be far more complicated. Of course, any task must be scheduled at some point for system success,
so there must be some non-starvation guarantee. 

\subsection{Real-time models}

A real-time system is a collection of tasks, and can be of different real-time models (\gls{SRT},
\gls{HRT}, best-effort). 
For analysis, each task in a
real-time system is modelled as an infinite series of \emph{jobs}. Each job represents a computation
with a deadline. 
One practical model in common use is the \emph{sporadic task model}~\citep{Mok:phd}, which can
model both \emph{periodic} and \emph{aperiodic} tasks and maps well onto typical control systems. 

\begin{listing}
\begin{minted}{c}
for (;;) {
  // job arrives
  doJob();
  // job completes before deadline
  // sleep until an event triggers the next job
  sleep();
}
\end{minted}
\caption{Example of a basic sporadic real-time task.}
\label{list:sporadic}
\end{listing}


The sporadic task model considers a real-time system as a set of tasks,
$\tau_{1},\tau_{2},\ldots,\tau_{n}$.
Each $\tau_{i}$ refers to a specific task in the task set, usually each for a different functionality.
The infinite series of jobs is per task, and represented by $\tau_{i1},\tau_{i2},\ldots,\tau_{in}$. Each task has
a \gls{WCET}, $C_{i}$, and period, $T_{i}$, which represents the minimum inter-arrival time
between jobs. When a $T_{i}$ has passed and a job can run again, it is said to be \emph{released}.
When a job is ready to run, it is said to have \emph{arrived} and when a job finishes and blocks until
the next arrival, it is said to be \emph{completed}. 

For periodic tasks, which are commonly found in control systems, jobs release and arrive at the same
time; they are always ready to run once their period has passed subject to \emph{jitter}, where the
release time is greater than the arrival time. Importantly, a late job-release does not alter the
deadline of the job, which remains relative to the arrival time. Sporadic tasks are more useful in
modelling interrupt driven tasks, where arrival times are unknown.

\begin{table}[b]
\rowcolors{2}{gray!25}{white}
\centering
\begin{tabularx}{\textwidth}{cX}\toprule
    \emph{Notation} & \emph{Meaning} \\\midrule
    $\tau$               & A task set. \\
    $\tau_{i}$           & A specific task in a task set \\
    $\tau_{ij}$          & A specific job of a specific task set. \\
    $T_{i}$           & The minimum period of a task, the minimum time between job releases. \\
        $C_{i}$       & The \gls{WCET} of a task ($C_{i} \leq T_{i}$). \\
    $U_{i}$           & The maximum utilisation of task $i$, which is $\frac{C}{T}$ \\
    $D_{i}$           & The relative deadline of task $i$ \\
    $t_{ij}$          & The release time of the $j$th job of task $i$ \\
    $d_{ij}$          & The deadline for the $j$th job of task $i$. \\
    $a_{ij}$          & The arrival time of the $j$th job of task $i$, $a_{ij} \geq t_{ij}$ \\
    $n$               & The number of tasks in a task set\\
    \bottomrule
    \end{tabularx}
    \caption{Parameters and notation for the sporadic task model.}
    \label{t:notation}
\end{table}

Jobs must complete before their period has passed ($C_{i} \leq T_{i}$), as the model does not
support jobs that run simultaneously. Tasks with interleaving jobs must be modelled as separate
tasks which do not overlap.  \Cref{list:sporadic} shows pseudocode for a sporadic task and
\Cref{t:notation} summarises the notation which is used throughout this thesis.

Sporadic tasks have deadlines relative to their arrival time, which may be \emph{constrained} or
\emph{implicit}.  An \emph{implicit} deadline means the job must finish before the period elapses.
\emph{Constrained} deadlines are relative to the release time, but before the period elapses.
Finally \emph{arbitrary} deadlines can be after the period elapses. Multiple sporadic tasks may be
released, that is to say, ready to run, at the same time, but they must be processed sequentially.

\begin{figure}[t]
	\begin{center}
		\leavevmode
		\includegraphics[width=\textwidth]{sporadic}
        \caption[Diagram of the sporadic task model.]{Diagram of the sporadic task model notation described in \cref{t:notation}, showing
        a task set $A$ with a single task $A_{0}$ where $T_{0} = 4$ and $C_{0} = 1$.}
		\label{fig:fp-schedule}
	\end{center}
\end{figure}


\section{Scheduling}
\label{sec:rt-scheduling}
% introduce scheduling terms, how we describe and evaluate schedulers

Simply put, scheduling is deciding which task to run at a specific time. In operating systems terms,
this is deciding which thread, or process to run on the processor next, and in the case of
multiprocessors, which processor to use. More formally, scheduling
is the act of assigning resources to activities or tasks~\citep{Baruah_CPV_96}, generally discussed
in the context of processing time, but is also required for other resources such as communication
channels.  A scheduling algorithm allocates time to tasks so that they all meet their deadlines,
subject to conditions such as utilisation bounds.
Real-time scheduling algorithms differ from their non-real-time equivalents in that they must guarantee that
all tasks not only receive sufficient access to the processor, but that all tasks meet their timing
requirements, \ie deadlines. 

Scheduling can be either \emph{static} or \emph{dynamic}. In a static or \emph{offline} schedule, 
the set of tasks is fixed and the schedule pre-calculated when the system is built, and does not
change once the system is running. 
Dynamic or \emph{online} scheduling occurs while the system is running, and the schedule is
calculated whenever a scheduling decision is required.
Most schedulers in common-use operating systems like Linux, are dynamic, online schedulers. 

All possible permutations of task parameters are not schedulable.
Thus, we refer to feasible and non-feasible task sets. 
A task set is \emph{feasible} if it can be scheduled by at least one scheduling algorithm such that
all temporal requirements are satisfied, and \emph{non-feasible} if no scheduling algorithm can be found
that satisfies the requirements.
An \emph{optimal} scheduling algorithm can schedule every feasible task set.
To test if a task set is schedulable by a scheduling algorithm, a \emph{schedulability test} is applied.
The complexity of schedulability tests is important for both dynamic and static schedulers. For
static schedulers, the test is conducted offline and not repeated, but the algorithm must complete
in a reasonable time for the number of tasks in the task set. Real-time, dynamic schedulers often conduct a
test each time a new task is submitted to the scheduler, or scheduling parameters are
altered, in order to check that the modified task set is schedulable. This test is referred to as an 
\emph{admission test}, and must be minimal in complexity to avoid undue overheads. 

There is an absolute limit on task sets that are feasible which can be derived from the total
utilisation. 
The \emph{total utilisation} of a task set is the sum of all the rates and must be less than the
total processing capacity of a system for all deadlines to be met.
For each processor in a system, this amounts to \cref{eq:1}.
\begin{equation}
    \label{eq:1}
	\sum\limits_{i=0}^n \dfrac{C_{i}}{T_{i}} \leq 1
\end{equation}
If the inequality does not hold, the system is considered \emph{overloaded}. Overload
can be \emph{constant}, in that it is all the time, or \emph{transient}, where the overload may be
temporary due to exceptional circumstances.

\begin{table}
    \centering
    \begin{tabular}{cccc} \toprule
        & \emph{$C_{i}$} & $T_{i}$ & $U_{i} $ \\ \midrule
			$ \tau_{1}$ & 1 & 4 & 0.25 \\
			$ \tau_{2}$ & 1 & 5 & 0.20 \\
			$ \tau_{3}$ & 3 & 9 & 0.33 \\
			$ \tau_{4}$ & 3 & 18 & 0.17  \\\midrule 
	$ U_{sum}(\tau)$ & &  & 0.95 \\ \bottomrule
	\end{tabular}
    \caption[A sample task set.]{A sample task set, adapted from ~\citep{Brandenburg:phd}}
	\label{tab:example_task_set}
\end{table}

Ideally, task sets are scheduled such that the total utilisation is equal to the number of
processors.  In practice, scheduling algorithms are subject to two different types of \emph{capacity
loss} which render 100\% utilisation impossible---algorithmic and overhead-related.
\emph{Algorithmic} capacity loss refers to processing time that is wasted due to the schedule used,
due to a non-optimal scheduling algorithm.  \emph{Overhead-related} capacity loss refers to time
spent due to hardware effects (such as cache misses, cache contention, and context switches) and
computing scheduling decisions.  Accurate schedulability tests should account for overhead-related
capacity loss in addition to algorithmic capacity loss.

Tasks often do not use all of their execution requirement, any execution time remaining at job
completion is referred to as \emph{slack}.
Slack can be a consequence of pessimistic \gls{WCET} values, or a result of varying job-lengths in
general. 
Sporadic tasks, where the actual arrival time varies from the minimum inter-arrival time, are also
sources of slack, as is jitter.
Many scheduling algorithms attempt to gain performance by reclaiming or stealing slack.

Scheduling algorithms are classed as either dynamic or fixed priority. A scheduling algorithm 
is \emph{fixed priority} if all the jobs in each task run at the same priority, which never changes. 
\emph{Dynamic priority} scheduling algorithms assign priorities to jobs not tasked, based on some criteria. 
There are two definitive dynamic scheduling  algorithms: \gls{FPRM}, which has fixed priorities, and
\gls{EDF}, which has dynamic priorities. Each is optimal for its respective scheduling class,  and both algorithms are defined along with
schedulability tests for the periodic task model in the seminal paper by \citet{Liu_Layland_73}. We
describe them briefly here, after covering the dominant static scheduling algorithm: cyclic
executives. 

\subsection{Cyclic executives}
\label{s:cyclic-executive}

A \emph{cyclic-executive} is an offline, static scheduler that dispatches tasks according to a pre-computed schedule, and is only
suitable for closed systems. Each task
has one or more entries in the pre-computed schedule, referred to as \emph{frames}, which specify a
\gls{WCET}. Frames are never preempted while running, resulting in a completely deterministic
schedule. The cyclic executive completes in a
\emph{hyperperiod}, which is the least common multiplier of all task periods. The \emph{minor cycle}
is the greatest common divisor of all the task periods.

Cyclic executives can, in theory, schedule task sets where the total utilisation is 100\%, as in \cref{eq:1}.
However, this only holds
in a task model where tasks can be split into infinite chunks, as tasks that do not fit into the
minor cycle must be split. This assumption is unrealistic as many tasks
cannot be split, and task switching is not without overheads. Calculating a cyclic schedule is
NP-hard, and must be done every time the task set changes.

Because cyclic executives are non-preemptible and deterministic, they cannot take advantage of 
over-provisioned \gls{WCET} estimates, therefore processor utilisation is low
for cyclic executives on modern hardware. 
One way to ameliorate the limitations of cyclic executives is to use a two-level scheduler with a
top level table that is static, and a second level of preemptive scheduling. When a table entry is
selected the 2nd level scheduler is then activated.

% survey of scheduling algorithms (FP, EDF, PFAIR)
\subsection{Fixed priority scheduling}
\label{s:fp}

As the name implies, \gls{FP} scheduling involves assigning fixed priorities to each task.
The scheduler is invoked when a job is released or a job ends, and the job with the highest priority is
always scheduled. Under real-time fixed-priority scheduling, priorities must be assigned such that
all deadlines are met. Two well-established priority-assignment techniques are \gls{RM} and
\gls{DM}.


\Gls{RM} priority assignment~\citep{Liu_Layland_73} allocates higher priorities to tasks with higher
rates---where \emph{rate} is determined by the period, as shown in \cref{eq:2}.
\begin{equation}
    \label{eq:2}
	\dfrac{1}{T_{i}}
\end{equation}

While \gls{RM} is only optimal for task sets with implicit deadlines, \gls{DM} priority
assignment~\citep{Leung_Whitehead_82} allocates higher priorities to tasks with shorter deadlines
and is optimal for implicit and constrained deadlines.
In both cases, ties are broken arbitrarily.
The \gls{FP} scheduling technique itself is not optimal, as it results in algorithmic capacity loss
and may leave up to 31\% of the processor idle. \cref{eq:3} shows the sufficient utilisation bound
for task sets on a uniprocessor for \gls{FPRM}, and \cref{eq:4} shows that the limit as the number of tasks in the task set ($n$) tends
towards infinity. For specific task sets, more accurate schedulability tests can be conducted via
response time analysis~\citep{Audsley_NRTW_93}.
\cref{f:fp-schedule} shows an example \gls{FPRM} schedule.

\begin{equation}
    \label{eq:3}
    \sum\limits_{i=0}^n \dfrac{C_{i}}{T_{i}} \leq n(2^{\frac{1}{n}}-1)
\end{equation}
\begin{equation}
    \label{eq:4}
    \lim_{n \to \infty}n(\sqrt[n]{2}-1) = \ln_{} 2 \approx 0.693147\ldots
\end{equation}

\subsection{Earliest Deadline First Scheduling}
\label{sec:background-edf}

The \gls{EDF} algorithm is theoretically optimal for preemptively scheduling a single resource, with no
algorithmic capacity loss; that is 100\% of processing time can be scheduled. This is because
\gls{EDF} uses dynamic priorities rather than fixed priorities. 
Priorities are assigned by examining the deadlines of each ready job; jobs with more immediate deadlines have higher priorities.
\Cref{f:edf-schedule} illustrates how the task set in \cref{tab:example_task_set} is scheduled by
\gls{EDF}, highlighting the places where tasks are scheduled differently from FPRM.

\begin{figure}[t]
	\centering
    \includegraphics[width=\textwidth]{fpschedule}
    \caption[An example FPRM schedule.]{An example FPRM schedule using the task set from Table \ref{tab:example_task_set}.}
    \label{f:fp-schedule}
\end{figure}
\begin{figure}[t]
	\centering	
	\includegraphics[width=\textwidth]{edfschedule}
    \caption[An example EDF schedule.]{An example EDF schedule using the task set from Table \ref{tab:example_task_set}.}
	\label{f:edf-schedule}
\end{figure}

\gls{EDF} is compatible with fixed-priority scheduling, as \gls{EDF} can be mapped to priority bands
in a fixed-priority system. Whenever an \gls{EDF} priority is selected, a second-level \gls{EDF}
scheduler dispatches the next task. This ``EDF in fixed-priority'' approach has been analysed in
detail~\citep{Harbour_Palencia_03} and is deployed in the Ada programming
language~\citep{Burns_Wellings:crtpa}, often used to build real-time systems.

\subsection{Earliest Deadline First versus Fixed Priority Scheduling}
\label{s:overload}

\gls{EDF} is less popular in commercial practice than \gls{FP} for a number of reasons, some which
are misconceptions, such as complexity of implementation and algorithmic capacity loss, and others
which represent concrete concerns, such as behaviour on overload. 

In terms of misconceptions, \gls{EDF}
is considered more complex to implement and to have higher overhead-related capacity loss. 
However, both of these points were debunked by \citet{Buttazzo_05}.  Although \gls{EDF} is difficult
and inefficient to implement on top of existing, priority-based \glspl{OS}, both schedulers
can be considered equally complex to implement from scratch.  \gls{FP} scheduling has higher
overhead-related capacity loss due to an increase in the amount of preemption.  This compounds the
algorithmic capacity loss, rendering \gls{EDF} a clear winner in from-scratch implementations in
terms of both properties.

Both algorithms behave differently under constant
overload. \gls{EDF} allows progress for all jobs but at a lower rate, while \gls{FP} will
continue to meet deadlines for jobs with higher \gls{RM} priorities, completely starving other
jobs. Whether these behaviours are desirable is subject to context, under
transient overload conditions both algorithms can cause deadline misses. \gls{FP} overload behaviour
is often preferred, as it will drop jobs from lower priority tasks deterministically. In contrast,
\gls{EDF} will drop jobs from all tasks, with no prioritisation. However, note that the
deterministic overload behaviour of \gls{FP} scheduling comes with a price: optimal priority
ordering is determined by the rate of jobs, not their importance to the system. 

Other comparisons between \gls{EDF} and \gls{FP} are the complexity of their schedulability tests.
\gls{EDF} and \gls{FP} scheduling both have pseudo-polynomial
schedulability tests under the sporadic task model~\citep{Abdelzaher_SL_04}, although \gls{EDF},
like \gls{FP}, under the periodic task
model\footnote{The periodic task model is the same as the sporadic task model, with the restriction
that deadlines must be equal to periods ($d = p$), while periods themselves are considered absolute,
not minimum.} has an $O(n)$ schedulability test~\citep{Baruah_RH_90}.  Like all pseudo-polynomial problems,
approximations can be made to reduce the complexity, although this comes with an error factor which
increases algorithmic capacity loss.

\subsection{Multiprocessors}

Both fixed and dynamic scheduling algorithms scheduling can be used on multiprocessor machines, either
\emph{globally} or \emph{partitioned}. Global schedulers share a single scheduling data structure
between all processors in the system, whereas partitioned schedulers have a scheduler per processor.
Neither is perfect: global approaches suffer from scalability issues such as hardware contention,
however partitioned schedulers require load balancing across cores.  Partitioning itself is known to
be a NP-hard bin-packing problem.  On modern hardware, partitioned schedulers outperform global
schedulers~\citep{Brandenburg:phd}.  For clustered multiprocessors a combination of global and
partitioned scheduling can be used; global within a cluster, and partitioned across clusters.

\section{Resource sharing}
\label{sec:resource-sharing-theory}

In the discussion so far we have assumed all real-time tasks are separate, and do not share resources.
Of course, any practical system involves shared resources. In this section we introduce the basics
of resource sharing, and the complexities of doing so in a real-time system.

Access to shared resources requires \emph{mutual exclusion}, where only one 
task is permitted to access a resource at a time, to prevent system corruption. Code that
must be accessed in a mutually exclusive fashion is called a \emph{critical section}. Generally
speaking, tasks lock access to resources, preventing other tasks from accessing that resource
until it is unlocked. However, many variants on locking protocols exist, and not all require mutual 
exclusion. Some permit $n$ tasks to access a section, or locks that behave differently for read and
write access.

Resource sharing in a real-time context is more complicated than standard resource sharing and
synchronisation, due to the problem of \emph{priority inversion}, which threatens the temporal
correctness of a system.  Priority inversion occurs when a low priority task prevents a high
priority task from running.  Consider the following example: if a low priority task locks a resource
that a high priority task requires, then the low priority task can cause the high priority task to
miss its deadline.  Consequently, all synchronised resource access in a real-time system must be
bounded, and analysis of a systems ability to meet deadlines must account for those bounds.

Bounded critical sections alone are not sufficient to guarantee correctness in a real-time
system. Consider the scenario outlined earlier, where a low priority thread holds a lock that a high
priority thread is blocked on.  If other, medium-priority tasks exist in the system, then the low
priority task will never run and unlock the lock, leaving the high priority task blocked for an
unbounded period.  This exact scenario caused the Mars Pathfinder to fault, causing unexpected
system resets~\citep{Mars_Pathfinder}.

In this section we provide a brief overview of real-time synchronisation protocols that
avoid unbounded priority inversion, drawn from \citet{Sha_RL_90}. First we consider uniprocessor
protocols before canvassing multiprocessor resource sharing.

\subsection{Non-preemptive critical sections}

Using the \gls{NCP}, preemption is totally disabled whilst in a critical section.  This approach blocks
all threads in the system while any client accesses a critical section.  Consequently, the bound on
any single priority inversion is the length of the longest critical section in the system.  Although
functional, this approach results in a lot of unnecessary blocking of higher priority threads.  The
maximum bound on priority inversion that a task can experience is the sum of the length of all
critical sections accessed by that task, as these are the only places that specific task can be
blocked while other tasks run. 

\subsection{Priority Inheritance Protocol}
\label{sec:pip}

In the \gls{PIP}, when a high priority task encounters a locked resource, it donates its priority 
to the task holding the lock and when the lock is released the priority is restored. 
This approach avoids blocking any higher priority threads that do not access this resource, and
works for both fixed and dynamic priority scheduling.
However, \gls{PIP} results in large systems overheads due to nesting, implementation complexity and
additional preemptions, and as a result has poor schedulability analysis.

To understand the additional preemptions inherent in \gls{PIP}, consider a task set with $n$ tasks,
$\tau_{1}, \ldots, \tau_{n}$, where each task's priority
corresponds to its index, such that the priority of $\tau_{i} = i$. The highest priority is $n$ and the
lowest is 1 and all tasks access the same resource. If $\tau_{1}$ holds the lock to that resource, then
the worst preemption overhead occurs if $\tau_{2}$ wakes up, elevating the priority of $\tau_{1}$ to 2. Subsequently,
each task wakes up in increasing priority order, each preempting $\tau_{1}$ until its priority reaches
$n$ resulting in $n$ total preemptions. 

Another disadvantage of \gls{PIP} is that deadlock can occur if resource ordering is not used.


\subsection{Immediate Priority Ceiling Protocol}
\label{sec:hlp}
\label{sec:ipcp}

Under \gls{IPCP}, also known as the highest lockers' protocol, resources are assigned a
\emph{ceiling} priority: the highest priority of all tasks that access that resource + 1.  When tasks
lock that resource, they run at the ceiling priority, removing the preemption overhead
of \gls{PIP}.

The disadvantage of \gls{IPCP} is that all priorities of task that access locked resources must be known \emph{a
priori}.  Additionally, if priority ceilings are all set to the highest priority, then behaviour
degrades to that of \gls{NCP}. Finally, this protocol allows intermediate priority tasks that do not need
the resource to be blocked unnecessarily. 

\subsection{Original Priority Ceiling Protocol}
\label{sec:opcp}

The \gls{OPCP} combines the previous two approaches, and avoids deadlock, excessive blocking and
excessive preemption. In addition to considering the priorities of tasks, \gls{OPCP} introduces
a dynamic global state referred to as the \emph{system ceiling}, which is the highest
priority ceiling of any currently locked resource. Like \gls{IPCP}, the \gls{OPCP} requires that all
priorities of tasks that lock resources be known \emph{a priority}. Under \gls{OPCP}, when a task locks a resource, its priority 
is not changed until another task attempts to acquire that resource, at which point the resource
holder's priority is boosted using priority inheritance. By delaying the priority boost the excessive 
preemption of \gls{PIP} is avoided. Additionally, tasks can only lock resources
when their priority is equal to the system ceiling, otherwise they block until this condition is
true, thus avoiding the risk of deadlock. \gls{OPCP} results in less blocking overall than \gls{IPCP},
however requires global state (the system ceiling) to be tracked across all tasks,
increasing the complexity of an implementation.

\subsection{Stack Resource Protocol}

Ceiling-based protocols are only appropriate for \gls{FP} schedulers, however the
\gls{SRP}~\citep{Baker_91} is provides similar functionality for \gls{EDF}. Under the \gls{SRP}, 
all tasks are assigned \emph{preemption levels} and all resources are assigned ceilings, which are
derived from the maximum preemption-level of tasks accessing those resources.
Similar to \gls{OPCP}, a system ceiling is also maintained, and is the maximum active preemption
level of
all tasks currently executing in the system. \gls{SRP} works by preventing preemption: a task is only allowed to preempt the system when two
conditions are met: that tasks absolute deadline must be less than the currently executing task, and
its preemption level must be higher than the current system ceiling. 

\subsection{Lock free algorithms}

Lock-free data structures can be used to avoid locks and priority inversion altogether,
on a uniprocessor this is achieved with
atomic operations like compare-and-swap, which are either supported by hardware or provided by disabling preemption. 
Lock-free data structures allow concurrent access to the same memory, however interleaved access can
result in a given operation being unbounded due to the failure of atomic retries. Although this
seems incompatible with real-time systems, \citet{Anderson_RJ_97b} show that lock free approaches can
be bounded and occur less overhead than wait-free or lock-based schemes.

However, given our approach uses formal verification, we do not consider lock-free options further,
as interleaving of program execution results in a state-space explosion and remains a challenge 
for verification on a large scale. 

\subsection{Summary}

\begin{figure}[ht]
  \centering
  \setlength{\unitlength}{1mm}
  \begin{picture}(50,25)(-5,-5)
    % WHOA! My first use of the picture environment in 25 years ;-)
    \thicklines
    \put(-5,0){\vector(1,0){50}}
    \put(7,-4){Amount of priority inversion}
    \put(0,-5){\vector(0,1){25}}
    \put(-4.5,2){\rotatebox{90}{Complexity}}
    \put(2,15){OPCP}
    \put(12,3){IPCP}
    \put(25,15){PIP}
    \put(35,1.5){NCP}
  \end{picture}
  \caption[Comparison of RT locking protocols.]{Comparison of real-time locking protocols based on
    implementation complexity and priority inversion bound.}
  \label{f:locking}
\end{figure}

\Cref{f:locking} compares the different uniprocessor locking protocols, showing that \gls{OPCP}
provides the lowest bound on priority inversion; however is also the most complicated to implement.
\gls{NCP} on the other hand, is the simplest to implement but exhibits the worst priority inversion
behaviour, with \gls{PIP} and \gls{IPCP} falling between the two.  \gls{IPCP} provides minimal
implementation complexity but requires a policy on priority assignment to be in place in the system.

\subsection{Multiprocessor locking protocols}

Resource sharing on multiprocessors is far more complicated than the single processor case and still
a topic of active research~\citep{Davis_Burns_11}. Of course, uniprocessor techniques can be used for resources that are
local to a processor, but further protocols are required for resources shared across cores (termed
\emph{global} resources).

Protocols for multiprocessor locking are either spinning- or suspension-based;
\emph{spinning} protocols spin on shared memory; \emph{suspension} protocols block the
task until the resource is available, such that other tasks can use the processor during that time.
Spin-lock protocols are effective for short critical sections, but once the critical section exceeds
the time taken to switch to another task and back, semaphore protocols are more efficient. 
\citet{Brandenburg_CBLA_08} find that for small, simple objects, non-blocking algorithms are
preferably, and that wait-free or spin-based approaches are better for large or complex objects,
in a real-time context with few cores. 
However, for systems with many-cores, spin-lock approaches do not scale beyond a small number
(\textasciitilde10)
cores~\citep{Clements_KZ_13}. 
Suspension-based approaches suffer extra preemptions in the worst case, while spin-locks do not.
With respect to worst-case bounds, suspension based protocols do not fair any better than spin-lock
approaches in real-time systems.

Multiprocessor locking protocols differ depending on the scheduling policy across cores; in
partitioned approaches, priorities on different cores are not comparable, meaning existing protocols
do not work. While the protocols we have examined so far can be used under global scheduling,
\citet{Brandenburg:phd} showed that partitioned approaches suffer far less cache overheads than
global scheduling. 

The \gls{MPCP}~\citep{Rajkumar_90} is a modified version of \gls{OPCP} for multiprocessors. It is a
suspension-based protocol that works by boosting task priorities. Tasks run at the highest priority
of any task that may access a global resource, and waiting tasks block in a priority queue. Nested
access to global resources is disallowed. The \gls{MSRP}~\citep{Gai_DL_03} is a spin-lock based
protocol, which can be used for \gls{FP} and \gls{EDF} scheduling. \gls{MSRP} uses the \gls{SRP} locally,
combined with \gls{FIFO} spin-locks which guard global resources. 

Multiprocessor real-time locking protocols are an extensive field of research, and 
many more sophisticated locking protocols exist, however we do not survey them here.

\section{Operating systems}
\label{sec:background-operating-systems}

An \gls{OS} is a software system that interfaces with hardware and devices in order to present a
common interface to applications.  The \emph{kernel} is the part of the operating system that
operates with privileged access to the processor(s) in order to safely perform tasks that allow
applications to run independently of each other.

Common \glspl{OS}, such as Windows, MacOS and Linux, are \emph{monolithic} operating systems,
which means that many services required to run applications are inside the kernel.  A \emph{microkernel}
attempts to minimise the amount of code running in the kernel in order to reduce the amount
of necessarily
trusted code.  Figure \ref{fig:os-microkernel} illustrates the difference between monolithic
\glspl{OS} and microkernels.  Modern microkernel implementation is guided by the minimality
principle~\citep{Liedtke_95} which aims to provide minimal mechanisms to allow resource servers to
function, leaving the rest of the policy up to the software running outside of the kernel. According
to the minimality principle, if a service does not need to be in the kernel to achieve its
functionality, it should not be in the kernel.

\begin{figure}
	\begin{center}
		\leavevmode
        \includegraphics[width=\textwidth]{os-microkernel.pdf}
		\caption{Structure of a microkernel versus monolithic operating system.}
		\label{fig:os-microkernel}
	\end{center}
\end{figure}

Monolithic operating systems provide scheduling, \gls{IPC}, device drivers, memory allocation, file
systems and other services in the kernel, resulting in a large \emph{trusted computing base}.

With respect to microkernels, interpretations of the minimality principle varies, 
consequently which services and utilities are included in the privileged kernel varies. In
larger kernels thread scheduling, memory allocation and some device drivers are included in the kernel.
For example, \selfour~\citep{Klein_EHACDEEKNSTW_09} contains scheduling and \gls{IPC}; 
\composite~\citep{Parmer:phd} does not provide a scheduler, or any blocking semantics, but does
provide \gls{IPC}. 

Microkernels are far more amenable for deployment in areas where security is a primary concern,
due to their small trusted computing base.
Services on top of the microkernel can be isolated and assigned different levels of trust, unlike
the services in a monolithic \gls{OS} which all run at the same privilege level such that a fault in
one service can compromise the entire system. 

Operating systems can run on each other in a process called \emph{virtualisation}, where the
underlying OS presents an interface imitating hardware. A \emph{hypervisor} is an operating system that can
run other operating systems on top of it, and operating systems running on the hypervisor are
referred to as \emph{guests}. Guest operating systems can be para- or fully-virtualised, where the
former involves modifications to the source of the guest. Modern hardware has virtualisation
extensions which improve virtualisation performance and reduce the need for para-virtualisation.
Both microkernels and monolithic kernels can also be hypervisors, although monolithic hypervisors
are often smaller than full OS counterparts, as they provide less functionality and rely on guest
OSes to provide most subsystems. 

\subsection{IPC}
\label{s:background-ipc}

\gls{IPC} is the microkernel mechanism for synchronous transmission of data and capabilities between
processes. Because the microkernel model provides services encapsulated into user-level servers,
\gls{IPC} is key to microkernel performance, as it is used more predominantly than in monolithic
\glspl{OS}. Originally, microkernels were criticised as impractical due to inefficient IPC
implementations of first-generation microkernels. However, this was demonstrated to be
false~\citep{Hartig_HLSW_97} due to the high cache footprint and poor design of the original
microkernels. Second-generation microkernels were built much leaner, with fewer services in the
kernel and fast, optimised code paths for IPC, referred to as \emph{fastpaths}. 
\emph{Third-generation} microkernels follow the pattern of minimality and speed, whilst also
promoting security as a first-class concern, which resulted in the incorporation of capability
systems.  

\subsection{Capabilities}
\label{s:b-capabilities}

Third-generation microkernels make use of \emph{capabilities}~\citep{Dennis_VanHorn_66}, an
established mechanism for fine-grained access control to spatial resources which allow for spatial
    isolation. A capability is a unique, unforgeable token that gives the possessor permission to access
an entity or object in system. Capabilities can have different levels of access rights, e.g. read,
write, execute etc. 

By combining access rights with object identifiers capabilities avoid the
confused deputy problem, a form of privilege escalation where a deputy program
acting on behalf of a client is  tricked into using
its own rights to manipulate a resource that the client would not normally have access
to~\citep{Hardy_88}. 

\subsection{Open vs. Closed Systems}

Operating systems can be built for open or closed systems.  An \emph{open system} is any system
where code outside of the control of the system designers can be executed. For example, modern
smart phones are open systems, given that users can install third-party applications.

A \emph{closed system} is the opposite; the system designers have complete control over all code
that will execute on the system.  The majority of closed systems are embedded, including those found
in cars, spacecraft and aircraft.

In general, there is a trend toward systems becoming more open; initial mobile phones were closed
systems.  This trend can be perceived from infotainment units in automobiles to televisions, where
the option to install third party applications is becoming more prevalent.  Allowing third-party
applications to run alongside critical applications on shared hardware increases the security
requirements of the system: critical applications must be isolated from third-party applications and
secure communications must be used between distributed components.  This is currently not the
general case, which has led to researchers demonstrating attacks on
cars~\citep{Checkoway_MKASSKCRK_11}.

Open systems are generally \emph{dynamic}---where resource allocations are configured at run-time
and can change---as opposed to closed systems which have \emph{fixed} or \emph{static} resource
allocation patterns.

%% introduce RTOSes
\subsection{Real-Time Operating Systems}

A \gls{RTOS} is an \gls{OS} that provides temporal guarantees, and can be microkernel-based or
monolithic.  Whilst some real-time systems run without operating systems at all, this approach is
generally limited to small, closed systems and is both inflexible and difficult to
maintain~\citep{Lui_AACBBBCLM_04}.

In a general purpose \gls{OS}, time is shared between applications with the aim of providing
\emph{fairness}, where applications share the processor equally.  This fairness is not divided into
equal share, but weighted, such that some applications are awarded more time than others in order
to tune overall system performance.  The \gls{OS} itself is not directly aware of the timing needs of
applications.

In an \gls{RTOS}, fairness is replaced by the need to meet deadlines. As a result, time is promoted
to a first class resource~\citep{Stankovic_88}.

Time being an integral part of the system affects every other part of the \gls{OS}.  For example, in
an \gls{RTOS}, one application having exclusive access to a resource cannot be allowed to cause a
deadline miss.  Similarly, the \gls{RTOS} itself cannot cause a deadline miss.  This means that all
operations in the \gls{RTOS} must either be bounded with known {\gls{WCET}} or the \gls{RTOS} must
be fully preemptible, with very small, bounded critical sections.  However, it must be noted that a fully preemptible \gls{OS} is completely
non-deterministic, in that interrupts can cause unpredictable interleaving of operations which lead
to state-space explosions in current verification techniques, making correctness impractical to guarantee~\citep{Blackham_TH_12}.  The overheads
of \gls{RTOS} operations like interrupt handling and context switching must also be considered when
determining whether deadlines can be met.

Traditional \glspl{RTOS}, and the applications running on them, require extensive offline analysis
to guarantee that all temporal requirements are met.  This is done by using scheduling algorithms,
schedulability and response time analysis,
\gls{WCET} analysis, and resource sharing algorithms with known real-time properties.

\subsection{Resource kernels}
\label{sec:resource-kernels}

Resource kernels are a class of \gls{OS} that treat time as a first class resource, by providing
timely, guaranteed access to system resources.  In a resource kernel, a reservation represents a
portion of a shared resource, like processor, or disk bandwidth.  Unlike traditional real-time
operating systems, resource kernels do not trust all applications to stay within their specified
resource bounds: resource kernels enforce them, preventing misbehaving applications from interfering
with other applications and thus providing temporal isolation.

In the seminal resource kernel paper, \citet{Rajkumar_JMO_98} outline four main goals that are
integral to resource kernels:
\begin{description}
    \item[G1: Timeliness of resource usage] Applications must be able to specify resource
        requirements that the kernel will guarantee.  Requirements should be dynamic: applications
        must be able to change them at run-time, however the kernel should ensure that the set of
        all requirements can be admitted.
    \item[G2: Efficient resource utilisation] The mechanisms used by the resource kernel utilise
        available resources efficiently and must not impose high utilisation penalties.
    \item[G3: Enforcement and protection] The kernel must enforce resource access such that rogue
        applications cannot interrupt the resource use of other applications.
    \item[G4: Access to multiple resource types] The kernel must provide access to multiple resource
        types, including processing cycles, disk bandwidth, network bandwidth and virtual memory.
\end{description}

In another paper, \citet{deNiz_LSR_01} outline the four main mechanisms that a resource kernel must
provide, in order to implement the above concepts.

\begin{description}
	\item[Admission] check that all resource requests can be scheduled (\textbf{G1}).
	\item[Scheduling] implements the dynamic allocation of resources according to reservations (\textbf{G1, G2}).
	\item[Enforcement] limit the consumption of the resources to that specified by the
        reservation (\textbf{G3}).
	\item[Accounting] of reservation use, to implement scheduling and enforcement (\textbf{G1, G2, G3}).
\end{description}

\label{p:resource-kernel-resource-sharing}
In order to share resources in a resource kernel, avoiding priority inversion becomes a more
complicated problem.  \citet{deNiz_LSR_01} outline three key policies that must be considered when
handling resource sharing in reservation-based systems:

\begin{description}
    \item[Prioritisation] What (relative) priority is used by the task accessing the shared resource (and under what conditions)?
    \item[Charging] Which reservation(s), if any, gets charged, and when?
    \item[Enforcement] What happens when the reservations being charged by the charging policy expire?
\end{description}

Resource kernels of the past were implemented as monolithic operating systems, where all system services
and drivers are provided by the kernel. However, nothing prevents the application of resource-kernel
principles to microkernel-based systems, although not all the mechanisms of a resource kernel are
suitable for inclusion in the microkernel itself: some can be provided by user-level middle-ware.  This
is because core resource kernel concepts contain both policy and mechanism.  We argue that the
microkernel should provide resource kernel mechanisms such that a resource kernel can be built with
a microkernel, but policy should be left up to the system designer, as long as it does not result in
performance or security concessions.

\section{Summary}

In this chapter we have briefly covered the core real-time theory that this thesis draws upon.
We have defined operating systems, and introduced the concepts that inform the design of resource kernels.
In the next chapter we will survey how these can be combined to achieve isolation and asymmetric protection for mixed-criticality systems.
