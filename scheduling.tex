\chapter{Temporal Isolation \& \\ Asymmetric Protection}
\label{chap:scheduling}

Mixed-criticality systems at their core require isolation: isolation as strong as that provided by
physically isolated systems, meaning if one sub-system fails it cannot affect other sub-systems.  Isolation
can be divided into two categories of resources: spatial and temporal. \emph{Spatial} resources include 
devices and memory, where isolation can be achieved using the \gls{MMU} and \IO\glspl{MMU}.
\emph{Temporal} isolation of resources is more complicated, and forms the focus of this chapter, where
we survey the relevant literature.
A system is said to provide \emph{temporal isolation} if temporal behaviour of one task cannot cause
temporal faults in another, independent task. 

What does isolation mean in a fully- or over-committed system, where there is no slack time 
to schedule? What if there simply is not enough time? One could argue that systems should be
over-provisioned to avoid such a scenario.
However, in the presence of \gls{SRT} and best-effort tasks which may be low in
criticality, this requirement is too strong. Instead, we must explore mechanisms for \emph{asymmetric
protection}, where high criticality tasks can cause a failure in low criticality tasks, but not vice
versa.

Much of the background examined in the previous chapter (\cref{sec:real-time-theory})
made the assumption that tasks would not exceed a declared \gls{WCET} or critical section bound. 
Many existing real-time systems run either one application, or multiple applications of the same
criticality, meaning each application that is running is certified to the same level.  This means
that all applications are trusted: trusted to not crash, and trusted to not overrun their deadlines.
If one application does overrun its deadline or use more processing time than specified by its
\gls{WCET}, guarantees are no longer met. 

Tasks can be untrusted for many reasons including:
\begin{itemize}
    \item sporadic tasks with inter-arrival times that are event driven will not necessarily
      have device drivers which guarantee the inter-arrival time;
    \item the task may have an unknown or unreliable \gls{WCET};
    \item the system may be open and the task from an untrusted source;
    \item the task may be low criticality and therefore not certified to a high level.
\end{itemize}

While large amounts of research have been devoted to addressing the challenges of scheduling
mixed-criticality systems, insufficient research has been conducted into
\glspl{OS} that practically enable the construction of mixed-criticality systems. 
A major barrier is the issue of trust: real-time tasks are often
assumed to perform correctly and safely, an assumption that no longer holds when tasks may have
different levels of certification and criticality.  However, much research has looked into the scheduling of
aperiodic tasks, which by definition cannot be trusted to follow a specific schedule, or abide by
their estimated minimum inter-arrival time. Further applicable research examines the scheduling of
\gls{SRT} and best-effort tasks along with \gls{HRT} tasks. Consequently, we examine scheduling methods for
these types of systems. 

Neither \gls{FP} nor \gls{EDF} scheduling
approaches discussed so far provide temporal isolation, although both can be adapted to do so.  In
this chapter we examine the techniques used by the real-time community to achieve temporal
isolation.

\section{Scheduling}


\subsection{Proportional-share schedulers}
\label{s:pfair}

Proportional share schedulers provide temporal isolation, as long as the system is not overloaded,
although this class of schedulers is based on achieving scheduling fairness between tasks, rather
than running untrusted tasks which may exceed their execution requirement. 

Recall that fairness is not a central property of scheduling in a \acrlong{RTOS}. However, one approach
for real-time scheduling is to specify a set of constraints that attempt to provide fairness and
also satisfy temporal constraints.  These are referred to as \emph{proportional share} algorithms,
which allocate time to tasks in discrete sized quanta. Tasks in proportional share schedulers are assigned 
weights according to their rate, and those weights determine the share of time for which each task 
has access to a resource.

While proportional share algorithms are applied to many scheduling problems, they apply
well to real-time scheduling on one or more processors.
Unlike other approaches to real-time scheduling, proportional share schedulers have the explicit property of guaranteeing a rate of progress for all tasks in the system.

\citet{Baruah_CPV_96} introduced the property \emph{proportionate fairness} or \emph{Pfair} as a
strong fairness property for proportionate share scheduling algorithms.  For a schedule to be Pfair,
then at every time $t$ a task $T$ with weight $T_{w}$ must have been scheduled either $\lceil T_{w}
. t \rceil$ or $\lfloor T_{w}.t \rfloor $ times.  \emph{Early-Release fair} or
\emph{ERfair}~\citep{Anderson_Srinivasan_04} is an extension of the Pfair property that allows tasks to
execute before their Pfair window, which can allow for better response times.

Pfair scheduling algorithms break jobs into sub-jobs that match the length of a \emph{quantum},
which is a fixed, discrete length of time defined by the system.
Real-time and non-real time tasks are treated similarly.
When overload conditions exist, the rate is slowed for all tasks.

Pfair scheduling algorithms are good theoretically but do not perform well in practice; they incur
large overhead related capacity loss due to an increased number of context
switches~\citep{Abeni_Buttazzo_04}. Additionally, since
scheduling decisions can only be made at quantised intervals, scheduling is less precise in
proportionate fair systems.  This problem can be exacerbated by critical sections, which may last
longer than a single quantum.  \citet{Stoica_AKBGP_96} propose defining arbitrary quanta sizes based
on maximum critical section size, however quanta size decreases the accuracy of the scheduler.
Additionally, it may not be possible to have \emph{a priori} knowledge of critical section size, especially
in a soft real-time system where it is not worth conducting \gls{WCET} analysis or execution time
is dependent on exterior factors, such as network behaviour.

One early uniprocessor Pfair scheduling algorithm is earliest-eligible deadline first, presented in
\citet{Stoica_AKBGP_96}.  PD$^{2}$~\citep{Srinivasan_Anderson_06} is a more recent Pfair/ERfair
scheduling algorithm that is theoretically optimal for multiprocessors under \gls{HRT} constraints,
although only under the assumption that process preemption and migration are free.

Recall that temporal isolation means that tasks should not be able to interfere with the temporal
behaviour of other tasks in the system.  Proportionate fair systems provide temporal isolation as
part of their fairness property, unless the system is overloaded, at which point the rate of all
tasks will degrade. Pfair schedulers by definition do not support asymmetric protection.

\subsection{Isolation with EDF schedulers}

Temporal isolation in \gls{EDF} scheduling has been explored thoroughly in the real-time discipline.
We outline the most dominant approaches in this section.

\subsubsection{Robust earliest deadline scheduling}

One early approach to temporal isolation with \gls{EDF} scheduling attempts to extend the 
algorithm to allow overload conditions to be handled with respect to a value. Robust earliest deadline
scheduling~\citep{Buttazzo_Stankovic_93} assigns a value to each task set, and will drop jobs from
low-value tasks under overload. If the system returns to non-overload conditions, those tasks are
scheduled again. This is a very early version of asymmetric protection.
The algorithm is optimal, however this is only the case if
scheduler overhead is excluded.  Since the algorithm has O($n$) complexity in the number of
tasks, the authors recommend using a dedicated scheduling processor such that overhead will not
affect the timing behaviour -- but this is not suitable for embedded systems, where the goal is to
minimise the number of processors, not increase them.

\subsubsection{Constant-bandwidth servers}

In the real-time community, the term \emph{server} is used to describe virtual time sources, where
an intermediate algorithm monitors task execution and, using that information, prevents task(s)
guarded by the server from exceeding specified temporal behaviour. These algorithms are integrated
into the scheduler.

\Gls{CBS} is a technique for scheduling \gls{HRT} and
\gls{SRT} tasks and providing temporal isolation~\citep{Abeni_Buttazzo_04}. Under \gls{CBS}, \gls{HRT} tasks are scheduled using an \gls{EDF}
scheduler, but \gls{SRT} tasks are treated differently as \gls{EDF} does not handle overload
reasonably.
Instead, a \gls{CBS} is assigned to each \gls{SRT} task.  Each \gls{CBS} has a bandwidth assigned to
it, and breaks down \gls{SRT} jobs into sub-jobs such that the utilisation rate of the task does not
exceed the assigned bandwidth.  Any sub-job that will cause the bandwidth to be exceeded is
postponed, but still executed.

\gls{CBS} stands out from previous server-based approaches like the total bandwidth
server\citep{Spuri_Buttazzo_94} as it does not require a \gls{WCET} or a minimum
bound on job inter-arrival time, making it much more suitable for \gls{SRT} tasks.  Implementation
wise, \gls{CBS} has less system overheads than Pfair schedulers.

Many extensions exist for \gls{CBS} to improve functionality.  \citet{Kato_IR_11} extend \gls{CBS} to
implement \emph{slack donation}, where any unused bandwidth is given to other jobs.  In
~\citep{Craciunas_KPRS_12}, \gls{CBS} is extended such that bandwidths are variable at run-time.
\citet{Lamastra_LA_01} introduce bandwidth inheritance across CBS servers applied to different
resources, providing temporal isolation for additional resources other than processing time.

\subsubsection{Rate-based EDF}

\Gls{RBED} schedulers explicitly separate the resource allocation and dispatching
(choosing which thread to run) in order to provide flexibility in timeliness requirements supported
by the scheduler.  \Gls{RBED} ~\citep{Brandt_BLB_03} is an algorithm that implements such a
scheduler.  In \gls{RBED}, tasks are considered as either \gls{HRT}, \gls{SRT}, best-effort or
rate-based.  Tasks are modelled using an extension of the periodic task model, allowing any job of a
task to have a different period.  If rate-based or \gls{HRT} tasks cannot be scheduled at their
desired rate they are rejected.  \gls{SRT} tasks are given their rate if possible with the option to
provide a quality of service specification.  Processor time reservations can be used to make sure
best-effort tasks are allowed some execution time.  Otherwise, they are allocated slack time unused
by SRT and HRT tasks.  Either way, best-effort tasks are scheduled by assigning them a rate that
reflects how they would be scheduled in a standard, fair, quantum-based scheduler.  Based on the
rates used, \gls{RBED} breaks tasks down and feeds them to an \gls{EDF} scheduler to manage
processing time.  Rates are enforced using a one-shot timer to stop tasks that exceed their
{\gls{WCET}}.  As tasks enter and leave the system, the rates of \gls{SRT} tasks will change.  Slack
time that occurs as a result of tasks completing before their deadlines is only donated to 
best-effort tasks, although the authors note that extensions should be able to donate slack to \gls{SRT}
tasks as well.  \Gls{RBED} is similar to the concept of \gls{CBS}, however it deals with separate types of
real-time tasks more explicitly.

\subsection{Isolation with FP schedulers}
\label{background:fp-isolation}

We will now
examine methods for temporal isolation in fixed-priority systems. 
Like \gls{CBS}, tasks are constrained by being encapsulating in a server, an algorithm which
rations out processing time, preventing specific scheduling parameters from being exceeded.
Approaches include polling servers, deferrable servers, sporadic servers, priority exchange servers
and slack stealing servers. 


\subsubsection{Polling servers}
\label{p:polling-servers}

Polling servers~\citep{Lehoczky_LS_87} wake every period,
to check if there are any pending jobs, then allows jobs to run until their budget is exhausted, at most. If there is no
task to run, the polling server will go back to sleep. That is, at time $t_{i}$, if there are no
tasks ready to execute, the server will sleep until $t_{i+1}$. This has the limitation that task latency
is just under the period $T$ -- if an event triggers a job just after the polling server wakes,
finds there is no work and returns to sleep, the pending job is delayed until the polling server
wakes again. 

\subsubsection{Deferrable Servers}
\label{p:ds} 

Unlike polling servers, \emph{deferrable
servers}~\citep{Lehoczky_LS_87, Strosnider_LS_95} preserve any unused budget across periods, although
the budget can never be exceeded.  This removes latency problems with polling servers, as deferrable
servers can wake any time a job is available. Unfortunately, deferrable servers do not provide 
true temporal isolation and can only guarantee that tasks will not exceed a maximum bandwidth under
periodic task constraints, where jobs arrive at the start of the period.
Under sporadic task constraints, where job arrival is only constrained by a minimum inter-arrival
time, jobs can execute back-to-back, thus exceeding their allocated scheduling bandwidth for any specific occurrence of
the period.  This occurs as deferrable servers replenish the budget to full at the start of each
period, and the budget can be used at any point during a task's execution. 

We demonstrate the problem with deferrable servers using the notation introduced in
\Cref{t:notation}. Consider a sporadic task with implicit deadlines in a task set, 
$A_{1}$, with jobs $A_{11}, A_{12}, \ldots, A_{1n}$. Each job in that task set has a deadline once the
period has passed: $d_{1j} = t_{1j} + T_{1}$. The problem occurs if the first job arrives at $a_{11}
= d_{11}-C_{1}$, such that it only completes at exactly the implicit deadline.  
Then a second job may arrive at the release time $d_{11}$ such that it runs back-to-back with the first
task, from $a_11$ to $d_{11} + C_{1}$, then the task has exceeded the bandwidth 
($\frac{C_{1}}{T_{1}})$ over a small window of time. As a result deadline misses can be caused in other
tasks, violating temporal isolation.

\subsubsection{Sporadic servers}
\label{p:sporadic} 

Sporadic servers~\citep{Sprunt_SL_89a} address the problems of
deferrable servers by scheduling multiple replenishment times, in order to preserve the property
that for all possible points in time $U_{i} \leq \frac{C_{i}}{T_{i}}$, known as the \emph{sliding window} constraint, which
is the condition that deferrable servers violate.  Each time a task is preempted, or blocks, a
replenishment is set for current time + $T_{i}$, for the amount consumed.  When no replenishments are
available, sporadic servers have their priority decreased below any real-time task.  The priority is
restored once a replenishment is available.  While this approach addresses the problems of deferrable
servers, the implementation is problematic as the number of times a thread is preempted or blocked
is potentially unbounded.  It is also subject to capacity loss as tasks that use very small chunks
of budget at a time increase the interrupt load.  The bigger the bound on replenishments the less
accurate the sporadic server, but the more memory used resulting in degraded performance.

\subsubsection{Priority exchange servers}

Priority exchange servers~\citep{Sprunt_SL_89a, Spuri_Buttazzo_94} allow inactive, high-priority tasks to swap their
priorities with active, low-priority tasks, such that server capacity is not lost but used at a lower
priority. Implementations of priority exchange require control and access to priorities across an
entire system.

\subsubsection{Slack stealing}

Slack stealing~\citep{Ramos_Thuel_Lehoczky_93} is an approach that runs a scheduling
task at the lowest priority and tracks the amount of slack per task in the system.  As aperiodic
tasks arrive, the slack stealer calculates whether they can be scheduled or not based on the slack
in the system and current load of periodic tasks.  This method does not provide guarantees at all
for the aperiodic tasks, unless a certain bound is placed on the execution of periodic tasks.

\section{Mixed-criticality schedulers}
\label{sched:mixed-criticality-schedulers}

In the real-time community, much work has been conducted around a theoretical mixed-criticality task
model introduced by \citet{Vestal_07}.  In this model, a system has a specific range of criticality
levels, $L_{min}-L_{max}$ and a current criticality level $L_{now}$. Each task in the system is
assigned a criticality level, $L_{i}$, and has a vector of execution times of size $L_{i}-L_{min}$.
When the system undergoes a mode change, should $L_{i} \geq L_{now}$ the execution time required by
that task is set to the value in the vector corresponding to the new $L_{now}$. If a task's
criticality is less than $L_{now}$, it may be dropped or scheduled with much lower priority until
a criticality mode-switch increases $L_{now}$.

\citet{Vestal_07}'s model can be interpreted in several ways; as a form of graceful degradation; or as an
optimisation in the case where a system designer and \gls{CA} disagree on \gls{WCET}
estimates for a system, which results in tasks with two {\gls{WCET}} estimates, one very pessimistic one from
the \gls{CA} and a less pessimistic one from the system designers or automated tools. 
Theoretically, if the system designer could show that the higher estimates
of the \gls{CA} can be met by such a mode change, they could schedule less criticality tasks
when in a low criticality mode. 

None of the scheduling algorithms so far directly support mixed-criticality systems.  \gls{RBED} is
the closest, although it assumes a direct relationship between criticality and real-time model, with
the assumption that \gls{HRT} tasks are more critical than \gls{SRT} tasks which are more critical
than best-effort tasks. 

As a result
of this, a family of mixed-criticality schedulers exists that handle high criticality tasks with two
{\gls{WCET}} estimates, and low-criticality tasks.  The scheduling algorithm will always schedule
high-criticality tasks.  If high-criticality tasks finish before the lower \gls{WCET} estimate,
lower criticality tasks are also scheduled.  Otherwise, tasks of lower criticality may not be
scheduled at all. 

\subsection{Static mixed-criticality}

Static mixed-criticality schedulers are built for variants on \citet{Vestal_07}'s model.
Scheduling algorithms in this class are distinguished by a \emph{criticality mode-switch}
between two or more criticality levels,
which may result in low criticality tasks being dropped or de-prioritised in some way. 
Schedulers for this model of mixed-criticality have been developed and extensively studied for
\gls{FP}~\citep{Vestal_07, Pathan:phd} and \gls{EDF}~\citep{Baruah_BDMVS_11}, and further. 
 As this has been a very active topic, we refer the reader to \citet{Burns_Davis_17} for
an extensive background. 

However, while this model, and many variations upon it have been subject to much research
in the real-time community, questions have been raised as to its practicality in industry.
\citet{Ernst_DiNatale_16} claim that a \gls{CA} would be unlikely to accept multiple \gls{WCET}
definitions and state that the focus of mixed-criticality research should be on
providing systems for error handling and proof of isolation in order to make mixed-criticality
systems practical.

\subsection{MC$^2$}
\label{sec:sched-mc2}

MC$^2$~\citep{Herman_KMAJ_12} is a mixed-criticality two-level hierarchical scheduler 
for multiprocessors with low core counts (2--8) which defines levels of task, which are scheduled differently.
\emph{Level-A} tasks are high-criticality tasks which are scheduled with a cyclic executive (recall
\cref{s:cyclic-executive}), with parameters computed offline. If no level-A tasks are eligible to
run, including in slack left in cyclic, level-A frames, \emph{level-B} tasks are considered. Level-B
tasks are trusted, \gls{HRT} tasks scheduled with partitioned \gls{EDF}, where one \gls{EDF}
scheduler exists per processor.
The authors note that the scheduler at level-B could be swapped with \gls{FPRM}.
In the slack left by level-A and level-B tasks, level-C tasks can run. Level-C tasks are globally
scheduled with \gls{EDF}, using one scheduler queue for all cores. The intuition is that level-C
tasks are \gls{SRT} tasks which run in the slack time of the more critical level-A and level-B
tasks. Level-D tasks are considered to be best-effort tasks, and are scheduled in the remaining
slack from A,B and C. MC$^2$ also has optional \emph{budget enforcement} that can be applied to all
levels of task, which prevents tasks from exceeding a specified bandwidth. 

The MC$^2$ scheduler is an example of a combination of policies, where criticality and real-time
model are aligned, in a way that may not be appropriate for all systems.

\subsection{Zero Slack Scheduling}
\label{s:zero-slack-scheduling}

\Citet{deNiz_LR_09} propose a scheduling approach that can handle multiple levels of criticality,
called \gls{ZS} scheduling. \gls{ZS} scheduling is based on the fact that tasks rarely use their
\gls{WCET}.  This means that resource reservation techniques like \gls{CBS} without slack donation
result in low effective utilisation.  ZS scheduling takes the reverse approach: high criticality
tasks steal utilisation from lower criticality tasks.  This involves calculating a \gls{ZS} instant
---the last point at which a task can be scheduled without missing its deadline.  Under overload, the
\gls{ZS} scheduler makes sure that high criticality tasks are scheduled by their \gls{ZS} instant,
such that they cannot be preempted by lower criticality tasks.

Implementations of \gls{ZS} scheduling can be built using any priority-based scheduling technique,
however in the initial work, \gls{FP} with \gls{RM} priority assignment is used.  The
\gls{ZS}\gls{RM} scheduler is proved to be able to schedule anything that standard \gls{RM}
scheduling can, whilst maintaining the asymmetric protection property.  \gls{ZS} scheduling can be
combined with temporal isolation via bandwidth servers.

\gls{ZS} scheduling has been adapted to use a \gls{QoS} based resource allocation
model~\citep{deNiz_WSRR_12}, in the context of \glspl{AAV}. Many models of real-time systems assume
that \glspl{WCET} for real-time tasks are stable and can be calculated.  However, \glspl{AAV} have
complicated visual object tracking algorithms where \gls{WCET} is difficult to calculate, and
execution time varies with the number of objects to track.  In practice, \Citet{deNiz_WSRR_12} found
that \gls{ZS}\gls{RM} scheduling resulted in \emph{utility inversion} --- where lower utility tasks
prevent the execution of higher utility tasks.  Although assuring no criticality inversion occurred
with a criticality-based approach, under overload, some tasks offer more utility than others with
increased execution time.  As a result, the authors replace criticality in the algorithm with a
utility function.  Two execution time estimates are used for real-time tasks --- \gls{NCET} and
\gls{OCET}, each having their own utility.  The goal of the scheduler is to maximise utility, under
normal operation and overload. In practice, utility and criticality are system specific values that
allow for further prioritisation than simply the rate of the task, required by \gls{FPRM}.

\section{Resource sharing}

One of our goals is to allow tasks of different criticality to share resources.  While the resource
itself must be at the highest criticality of any task using it, this relationship is not
necessarily symmetric; low criticality systems should be able to use high criticality resources, as
long as their resource access is bounded. 
In this section we explore how resource reservations and real-time locking protocols interact, and
assess their suitability for mixed criticality systems.

So far in this chapter we have looked at real-time
theory for providing temporal isolation: usually in the form of isolation algorithms (termed servers
in real-time theory) which
ration out execution time, guaranteeing a maximum bandwidth or allowing aperiodic tasks to run in
the slack time of other tasks. Similarly, the resource sharing locking protocols of priority
inheritance and ceiling priorities are not designed to work if tasks misbehave: in all the
protocols, if a
task does not voluntarily release a resource, all other tasks sharing that resource will be
blocked. One cannot blindly apply a technique like polling servers directly to resource sharing, as if the
bandwidth expires while the resource is held no other task can access that resource.

Resource kernels, as introduced in \cref{sec:resource-kernels}, outline the policy decisions that
must be made when combining locking protocols and reservations: prioritisation, charging and
enforcement. 

Prioritisation, or what priority a task uses while accessing a resource, can be decided by any of
the existing protocols: \gls{OPCP}, \gls{IPCP}, \gls{PIP} or \gls{SRP}. Charging is more complex.
Notionally, when a thread is temporally contained by an isolation algorithm, that algorithm
corresponds to a reservation. Under normal execution, the reservation is charged when that thread
executes. However, if a several threads are vying for a resource, which thread's reservation should
be charged for the time executed accessing that resource> 

\citet{deNiz_LSR_01} describe the possible mappings
between reservations and resources consuming those reservations, which comes down to the following
choices:

\begin{description}
\item[Bandwidth inheritance] Tasks using the resource run on their own reservation.  If that
    reservation expires and there are other pending tasks, the task runs on the reservations of the
    pending tasks. 
\item[Reservation for the resource] Shared resources have their own reservation, which tasks use.
    This reservation must be enough for all tasks to complete their request.  Once again, if tasks
    are untrusted no temporal isolation is provided. 
\item[Multi-reserve resources] Shared resources have multiple reservations, and the resource
    actively switches between them depending on which task it is servicing. 
\end{description} 

Finally, the most relevant to mixed-criticality systems, is enforcement: if the reservation being
charged for a resource access is fully consumed, what should happen? Without an enforcement
mechanism, all tasks waiting to access that resource are blocked until more execution bandwidth is
rationed by the isolation algorithm. In the majority of cases, the bandwidth inheritance is used,
relying on the idea that any operations involving shared resources in real-time systems are bounded.
Then, depending on the prioritisation protocol, tasks must have enough budget to account for that
bound. This imposes a major restriction on shared resources: even in the case where soft-real time
access is required of those resources, bounds must be known.

\subsection{MC-IPC}
\label{sec:sched-mc-ipc}

\citet{Brandenburg_14} implements a multiprocessor \gls{IPC}-based protocol referred to as MC-IPC,
where shared resources are placed in resource servers accessed. In this scheme, the resources
themselves must be at the ceiling criticality of any task accessing those resources,
but all tasks do not have to be at that criticality level.
The protocol works by channelling all IPC requests through a three-level, multi-ended queue
where high-criticality tasks are prioritised over best-effort tasks. The protocol relies on bounded
resource access if a task exhausts its allocation while using the resource, combined with correct
queue ordering such that high-criticality tasks are not blocked by low-criticality ones. 

\section{Summary}

In traditional scheduling algorithms (\gls{EDF} and \gls{FPRM}), temporal isolation only exists as a
convention: tasks are expected to specify their parameters \emph{a priori}, and trusted to not
exceed those parameters, an approach which is not suitable for mixed-criticality systems, where
tasks of different levels of assurance need to share resources.

In this chapter we have covered theoretical techniques for temporal isolation, mostly derived from 
theoretical approaches to contain aperiodic tasks, which are unpredictable workloads. One approach
is to specify a processor share, or bandwidth, and use an algorithm such as \gls{CBS} or \gls{SS} to 
temporally contain a task. The majority of these algorithms can be used to implement processor
reservations.

We also examined several mixed-criticality schedulers, however all incorporated a great deal of
policy: schedulers that provide a criticality switch assume multiple \gls{WCET} estimates, and
that low-criticality tasks can be de-prioritised or dropped. MC$^2$ defines four levels of
scheduling and assumes that the highest criticality task are also hard real time, and that
best-effort tasks are less critical than \gls{SRT} tasks, while \gls{ZS}-scheduling is optimised for
a particular measure of utility. 

The mixed-criticality schedulers studied all assume that the criticality of a task is directly
related to the real-time strictness: for example, MC$^2$ prioritises \gls{HRT} over \gls{SRT}. 
While in
general critical tasks are \gls{HRT}, it is possible to have critical tasks that are \gls{SRT}, for
instance, object tracking algorithms whose \gls{WCET} depends on factors external to the software
system. Therefore, although the scheduling policies discussed can solve a specific problem, they are
not appropriate for all systems, and should not be mandated.

Finally, we looked at resource sharing, and how this interacts with processor reservations,
We saw that while there are existing policies for prioritisation and charging, the
majority of enforcement mechanisms are based on bandwidth inheritance. 

In the next chapter, we survey existing operating systems and systems techniques with respect to
temporal isolation capability, resource sharing, and asymmetric protection.
